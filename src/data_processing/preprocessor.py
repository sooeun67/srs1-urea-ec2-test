# ======================
# preprocessor.py
# ======================
"""
Preprocessing utilities for SKEP Urea Control System

ë³¸ ëª¨ë“ˆì€ config ê¸°ë°˜ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ ì œê³µí•©ë‹ˆë‹¤.
- [TEST ì „ìš©] ì¶”ì²œê°’ ê³„ì‚°ì„ ìœ„í•œ ì œí•œ ffill (í•„ìš” ì‹œ full index ìƒì„±)
- [TRAIN ì „ìš©] ëª¨í˜• í•™ìŠµ ë°ì´í„° ìƒì„±:
    1) ë¹„ì •ìƒ ê°€ë™ êµ¬ê°„ ì œì™¸ (INCINERATORSTATUS == 1 ì˜ Â±window_min ì œì™¸)
       - cfg.exclude_status_value, cfg.exclude_window_min
    2) ë³€ìˆ˜ë³„ ì„ê³„ê°’(global_threshold) ë°–ì˜ ê°’ â†’ ê²°ì¸¡ì¹˜
    3) ì‹œê°„ ì œì•½ ì„ í˜•ë³´ê°„ (ì•/ë’¤ ìœ íš¨ ê´€ì¸¡ì¹˜ê°€ limit_sec ì´ë‚´ì¸ ê²½ìš°ì—ë§Œ ë³´ê°„)
       - cfg.interpolate_limit_sec, cfg.interpolate_method
    4) (GP ì „ìš©) AI ìš´ì „ í•„í„° â†’ dedup â†’ sample â†’ dropna â†’ min_samples ì²´í¬
Config ì˜ì¡´ í•­ëª©
----------------
- CommonPreprocessingConfig:
    - column_config: ColumnConfig (í•„ìˆ˜)
    - resample_sec: int (full index ê°„ê²©/ffill ê³„ì‚°ì— ì‚¬ìš©)
    - global_threshold: Dict[str, Tuple[float, float]] (__post_init__ì—ì„œ ì‹¤ì»¬ëŸ¼ëª… í‚¤ë¡œ ì™„ì„±)
- TrainPreprocessingConfig / GPTrainPreprocessingConfig:
    - start_time / end_time (ì„ íƒ)
    - interpolate_limit_sec, interpolate_method
    - dedup, sample_size, random_state, dropna_required, min_samples
    - logger_cfg (LoggerConfig)
    - exclude_status_value: int  (ê¸°ë³¸ 1)
    - exclude_window_min: int   (ë¶„ ë‹¨ìœ„, ê¸°ë³¸ 40)
- InferPreprocessingConfig:
    - ffill_limit_sec
    - resample_sec

ColumnConfig ì˜ì¡´ ì»¬ëŸ¼ ì˜ˆì‹œ
--------------------------
- col_datetime (í•„ìˆ˜)
- col_o2, col_hz, col_inner_temp, col_outer_temp, col_nox (ì„ê³„ê°’ ê¸°ì¤€)
- col_incinerator_status

ì‚¬ìš© ì˜ˆ
-------
from config.preprocessing_config import GPTrainPreprocessingConfig, InferPreprocessingConfig
from config.column_config import ColumnConfig
from preprocess import Preprocessor

cc = ColumnConfig(...)
train_cfg = GPTrainPreprocessingConfig(column_config=cc, plant_code="SRS1")
infer_cfg = InferPreprocessingConfig(column_config=cc, plant_code="SRS1")

pp = Preprocessor()

df_train = pp.make_train_dataset(df_raw, train_cfg)
df_test  = pp.make_test_ffill(df_latest, infer_cfg, require_full_index=True)  # True ë˜ëŠ” False
"""

from __future__ import annotations

from dataclasses import dataclass, field
from typing import Iterable, Optional, Sequence, Tuple, List, Dict

import warnings
import numpy as np
import pandas as pd

from utils.logger import get_logger, LoggerConfig
from config.preprocessing_config import (
    CommonPreprocessingConfig,
    TrainPreprocessingConfig,
    GPTrainPreprocessingConfig,
    InferPreprocessingConfig,
)
from config.column_config import ColumnConfig
from config.model_config import GPModelConfig



__all__ = ["Preprocessor"]


# ============================================================
# ë‚´ë¶€ ìœ í‹¸
# ============================================================
# ì´ˆ ë‹¨ìœ„ë¥¼ pandas ë¹ˆë„ ë¬¸ìì—´ë¡œ ë³€í™˜ (ì˜ˆ: 5 â†’ '5s')
def _freq_from_seconds(sec: int) -> str:
    """
    ì •ìˆ˜(ì´ˆ) â†’ pandas ë¹ˆë„ ë¬¸ìì—´ë¡œ ë³€í™˜.

    Parameters
    ----------
    sec : int
        ìƒ˜í”Œë§/ë¦¬ìƒ˜í”Œë§ ê°„ê²©(ì´ˆ). 5 â†’ '5s'

    Returns
    -------
    str
        pandas ë¹ˆë„ ë¬¸ìì—´ (ì˜ˆ: '5s')

    Notes
    -----
    - 0 ë˜ëŠ” ìŒìˆ˜ ì…ë ¥ì€ ì—ëŸ¬ë¥¼ ë°œìƒì‹œí‚µë‹ˆë‹¤.
    """
    if sec <= 0:
        raise ValueError(f"resample_sec must be > 0, got {sec}")
    return f"{int(sec)}s"

# datetime ì»¬ëŸ¼ì„ pandas datetime ì‹œë¦¬ì¦ˆë¡œ ê°•ì œ ë³€í™˜
def _datetime_series(df: pd.DataFrame, col_datetime: str) -> pd.Series:
    """
    ì§€ì •ëœ datetime ì»¬ëŸ¼ì„ pandas datetimeí˜•ìœ¼ë¡œ ê°•ì œ ë³€í™˜í•´ ë°˜í™˜.

    Parameters
    ----------
    df : pd.DataFrame
        ì›ë³¸ ë°ì´í„°í”„ë ˆì„
    col_datetime : str
        datetime ì»¬ëŸ¼ëª…

    Returns
    -------
    pd.Series
        ë³€í™˜ëœ datetime ì‹œë¦¬ì¦ˆ (íƒ€ì… ë³´ì¥)

    Notes
    -----
    - ë³€í™˜ ì‹¤íŒ¨ ì‹œ pandasê°€ ê°€ëŠ¥í•œ ë²”ìœ„ì—ì„œ íŒŒì‹±í•©ë‹ˆë‹¤.
    - ì •ë ¬ì€ ìˆ˜í–‰í•˜ì§€ ì•Šìœ¼ë©°, í˜¸ì¶œ ì¸¡ì—ì„œ ì •ë ¬í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.
    """
    s = pd.to_datetime(df[col_datetime])
    return s

# full index(ê· ì¼ ì‹œê°„ ì¸ë±ìŠ¤) ìƒì„±í•˜ì—¬ ê²°ì¸¡ ì‹œì  í¬í•¨ ì¬ì¸ë±ì‹±
def _ensure_full_index(
    df: pd.DataFrame,
    col_datetime: str,
    freq: str,
    col_real_row: str,
) -> pd.DataFrame:
    """
    ê· ì¼ ì‹œê°„ ì¸ë±ìŠ¤(full index)ë¥¼ ìƒì„±í•˜ì—¬ ê²°ì¸¡ ì‹œì  í¬í•¨ ì¬ì¸ë±ì‹±.

    Parameters
    ----------
    df : pd.DataFrame
        ì…ë ¥ ë°ì´í„°í”„ë ˆì„(ë¯¸ë¦¬ col_datetime ê¸°ì¤€ ì •ë ¬ ê¶Œì¥)
    col_datetime : str
        datetime ì»¬ëŸ¼ëª…
    freq : str
        íƒ€ì„ìŠ¤í… ê°„ê²©(ex. '5s', '1min')

    Returns
    -------
    pd.DataFrame
        full indexë¡œ ì¬ì¸ë±ì‹±ëœ ë°ì´í„°í”„ë ˆì„.
        ê²°ì¸¡ ì‹œì ì€ NaNì´ í¬í•¨ë˜ë©°, ì´í›„ ffill/bfill ë“±ìœ¼ë¡œ ì²˜ë¦¬ ê°€ëŠ¥í•©ë‹ˆë‹¤.

    Notes
    -----
    - start~end ë²”ìœ„ëŠ” ì…ë ¥ ë°ì´í„°ì˜ ìµœì†Œ/ìµœëŒ€ ì‹œê°ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    - ë°˜í™˜ ì‹œ datetime ì»¬ëŸ¼ì„ ë³µì›í•©ë‹ˆë‹¤.
    """
    if df.empty:
        out = df.copy()
        out[col_real_row] = False
        return out

    src = df.copy()
    src[col_datetime] = pd.to_datetime(src[col_datetime])
    src = src.sort_values(col_datetime, ignore_index=True)

    start = src[col_datetime].min()
    end = src[col_datetime].max()
    full_index = pd.date_range(start=start, end=end, freq=freq)

    re = src.set_index(col_datetime).reindex(full_index)
    re = re.reset_index().rename(columns={"index": col_datetime})

    orig_ts = pd.Index(src[col_datetime].unique())
    re[col_real_row] = re[col_datetime].isin(orig_ts)

    return re

# ì§€ì • ì‹œê°„ í•œë„ ì´ë‚´ì—ì„œë§Œ ffill ì ìš©
def _limited_ffill(
    df: pd.DataFrame,
    col_datetime: str,
    freq: str,
    limit_seconds: int,
) -> pd.DataFrame:
    """
    'ì‹œê°„ ê¸°ì¤€' ì œí•œ ffill ìˆ˜í–‰.

    Parameters
    ----------
    df : pd.DataFrame
        ì…ë ¥ ë°ì´í„°í”„ë ˆì„
    col_datetime : str
        datetime ì»¬ëŸ¼ëª…
    freq : str
        ë°ì´í„°ì˜ ê· ì¼ ê°„ê²©(ì˜ˆ: '5s'). limit_secondsë¥¼ 'í–‰ ìˆ˜'ë¡œ ë³€í™˜ì— ì‚¬ìš©
    limit_seconds : int
        í—ˆìš©í•  ìµœëŒ€ ffill ì‹œê°„(ì´ˆ)

    Returns
    -------
    pd.DataFrame
        ì œí•œ ffill ì ìš© ê²°ê³¼(í–‰ ê¸°ì¤€ limit=âŒŠlimit_seconds/freqâŒ‹)

    Notes
    -----
    - pandas ffill(limit=N)ëŠ” 'í–‰ ê°œìˆ˜' ë‹¨ìœ„ì´ë¯€ë¡œ, ì´ˆâ†’í–‰ ê°œìˆ˜ë¡œ ë³€í™˜í•˜ì—¬ ì ìš©í•©ë‹ˆë‹¤.
    - limit_secondsê°€ 0 ì´í•˜ì´ë©´ ffillì„ ì ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    """
    out = df.set_index(col_datetime)
    max_rows = int(pd.Timedelta(seconds=int(limit_seconds)) / pd.Timedelta(freq)) - 1  # 20ì´ˆ ê¸°ì¤€ìœ¼ë¡œ ë§ì¶°ë‘ , 4ê°œ í–‰ ì´ì „ ë°ì´í„°ë¡œë¶€í„° ffill í•˜ê³ ì‹¶ìœ¼ë©´ DBì—ì„œ 25ì´ˆë¥¼ ê°€ì ¸ì˜¤ê²Œ ìˆ˜ì •í•´ì•¼í•¨
    max_rows = max(0, max_rows)
    if max_rows == 0:
        # ì œí•œ 0ì´ë©´ ffill ë¯¸ì ìš©
        out2 = out.copy()
    else:
        out2 = out.ffill(limit=max_rows)
    return out2.reset_index()

# ì•ë’¤ ìœ íš¨ ê´€ì¸¡ ì‹œê° ì°¨ê°€ limit_sec ì´ë‚´ì¸ ê²°ì¸¡ì ë§Œ ì„ í˜• ë³´ê°„
def _time_limited_interpolate(
    df: pd.DataFrame,
    col_datetime: str,
    target_cols: Sequence[str],
    limit_sec: int,
    method: str = "time",
) -> pd.DataFrame:
    """
    'ì•/ë’¤ ìœ íš¨ ê´€ì¸¡ ì‹œê° ì°¨'ê°€ limit_sec ì´ë‚´ì¸ ê²°ì¸¡ì ë§Œ ì„ í˜•ë³´ê°„ìœ¼ë¡œ ì±„ì›€.

    Parameters
    ----------
    df : pd.DataFrame
        ì…ë ¥ ë°ì´í„°í”„ë ˆì„(ì •ë ¬ë˜ì§€ ì•Šì•„ë„ ë‚´ë¶€ì—ì„œ ì •ë ¬)
    col_datetime : str
        datetime ì»¬ëŸ¼ëª…
    target_cols : Sequence[str]
        ë³´ê°„ ëŒ€ìƒ ì»¬ëŸ¼ ëª©ë¡(ìˆ˜ì¹˜í˜•)
    limit_sec : int
        ë³´ê°„ í—ˆìš© ì‹œê°„ í•œë„(ì´ˆ): ì´ì „/ë‹¤ìŒ ìœ íš¨ ì‹œê°ê³¼ì˜ ì°¨ê°€ ëª¨ë‘ ì´ë‚´ì—¬ì•¼ í•¨
    method : {'time', 'linear'}, default 'time'
        pandas.Series.interpolateì˜ ë°©ë²•

    Returns
    -------
    pd.DataFrame
        ì‹œê°„ ì œì•½ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ê²°ì¸¡ê°’ë§Œ ë³´ê°„ëœ ë°ì´í„°í”„ë ˆì„

    Notes
    -----
    - 'time' ë³´ê°„ì´ ë¶ˆê°€í•œ ê²½ìš°(ì˜ˆ: ë™ì¼ íƒ€ì„ìŠ¤íƒ¬í”„ ë¬¸ì œ) ìë™ìœ¼ë¡œ 'linear'ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.# (ìˆ˜ì •) ->  ë™ì¼ timestamp ë¬¸ì œë¼ëŠ” ì—ëŸ¬ë¡œê·¸ ë„ìš°ê³  drop_duplicates í•´ì„œ ì²«í–‰ë§Œ ë‚¨ê¸°ëŠ”ê±¸ë¡œ
    - ë³´ê°„ì€ limit_area='inside'ë¡œ ì–‘ë extrapolationì„ ë°©ì§€í•©ë‹ˆë‹¤.
    """
    out = df.copy()
    ts = _datetime_series(out, col_datetime)
    out[col_datetime] = ts
    out = out.sort_values(col_datetime).reset_index(drop=True)

    # ë™ì¼ timestamp ì²˜ë¦¬
    if out[col_datetime].duplicated().any():
        warnings.warn(
            "[TRAIN-COMMON] _time_limited_interpolate: ë™ì¼ timestamp ê°ì§€ â†’ "
            "drop_duplicates(keep='first') ìˆ˜í–‰", RuntimeWarning
        )
        out = out.drop_duplicates(subset=[col_datetime], keep="first").reset_index(drop=True)
        ts = out[col_datetime]  # ê°±ì‹ 

    for col in target_cols:
        if col not in out.columns:
            continue
        s = out[col]
        if not pd.api.types.is_numeric_dtype(s):
            continue

        # ì‹œê°„ì¶• ë³´ê°„
        s_time_indexed = pd.Series(s.values, index=ts)
        try:
            s_interp = s_time_indexed.interpolate(method=method, limit_area="inside")
        except Exception: # (ìˆ˜ì •) ->  ë™ì¼ timestamp ë¬¸ì œë¼ëŠ” ì—ëŸ¬ë¡œê·¸ ë„ìš°ê³  drop_duplicates í•´ì„œ ì²«í–‰ë§Œ ë‚¨ê¸°ëŠ”ê±¸ë¡œ
            # (ë³´ê°•) method='time' ì‹¤íŒ¨ ì‹œ linear ì¬ì‹œë„ (ì¤‘ë³µ ì œê±° ì´í›„ì—ë„ ì˜ˆì™¸ ì‹œ)
            warnings.warn(
                f"[TRAIN-COMMON] interpolate(method='{method}') ì‹¤íŒ¨ â†’ 'linear'ë¡œ ì¬ì‹œë„: col={col}",
                RuntimeWarning
            )
            # time ë³´ê°„ì´ ë¶ˆê°€í•œ ê²½ìš°(ì˜ˆ: ë™ì¼ timestamp ë¬¸ì œ) linearë¡œ fallback
            s_interp = s_time_indexed.interpolate(method="linear", limit_area="inside")

        # ì¸ë±ìŠ¤ ì¬ì •ë ¬
        s_interp_aligned = pd.Series(s_interp.values, index=s.index)

        # ì•/ë’¤ ìœ íš¨ ì‹œê°
        last_valid_time = pd.Series(ts.where(s.notna()), index=s.index).ffill()
        next_valid_time = pd.Series(ts.where(s.notna()), index=s.index).bfill()

        # ì‹œê°„ì°¨(sec)
        dt_prev = (ts - last_valid_time).dt.total_seconds()
        dt_next = (next_valid_time - ts).dt.total_seconds()

        # ë³´ê°„ í—ˆìš© ë§ˆìŠ¤í¬
        can_interp = ((dt_prev <= limit_sec) & (dt_next <= limit_sec)).fillna(False)

        # ê²°ì¸¡ & í—ˆìš© â†’ ë³´ê°„ ë°˜ì˜
        out_col = s.copy()
        idx = s.isna() & can_interp
        out_col[idx] = s_interp_aligned[idx]
        out[col] = out_col

    return out

# ============================================================
# EQ_Status ìœ í‹¸
# ============================================================
def _estimate_step_seconds(ts: pd.Series) -> float:
    """ì‹œê°„ ê°„ê²©(ì´ˆ) ì¶”ì •: median diff ì‚¬ìš©, ì‹¤íŒ¨ ì‹œ 1.0ì´ˆ."""
    try:
        diffs = ts.sort_values().diff().dropna()
        if diffs.empty:
            return 1.0
        step = diffs.median().total_seconds()
        return float(step) if step and step > 0 else 1.0
    except Exception:
        return 1.0

# ============================================================
# EQ_Status: ë“œë¡­ ì—†ì´ NaN ë§ˆìŠ¤í‚¹ + í”Œë˜ê·¸ ì„¸íŒ…
# ============================================================
def _apply_eq_status_mask(
    df: pd.DataFrame,
    col_datetime: str,
    sensor_cols,
    resample_sec: int,
    shift_sec: int,
    min_nan_block_sec: int,
    logger: logging.Logger,
    cc: ColumnConfig,
    col_flag: str,  # ColumnConfig.col_eq_status_filtered
) -> Tuple[pd.DataFrame, Dict[str, Any]]:
    """
    EQ_Status ê¸°ë°˜ ì²˜ë¦¬

    ë‹¨ê³„
    ----
    (A) ì¡°ê±´ 1~3 + ì‹œí”„íŠ¸: ëŒ€ìƒ ì„¼ì„œ íƒ€ê¹ƒ ì»¬ëŸ¼ì„ NaN ë§ˆìŠ¤í‚¹í•˜ê³ , ìƒˆë¡œ NaNì´ ëœ í–‰ì€ í”Œë˜ê·¸ True
    (B) ì—°ì† NaN â‰¥ ì„ê³„ê¸¸ì´: NaN/ë“œë¡­ ë³€ê²½ ì—†ì´ í”Œë˜ê·¸ë§Œ True
    (C) ëª¨ë“  EQ==0 ê°•ì œ: NaN/ë“œë¡­ ë³€ê²½ ì—†ì´ í”Œë˜ê·¸ë§Œ True

    ë°˜í™˜
    ----
    df_out : pd.DataFrame
        - ê°’ì´ ë³€ê²½ëœ DataFrame (ë‹¨, (B)(C)ëŠ” ê°’ ë³€ê²½ ì—†ìŒ)
        - í”Œë˜ê·¸ ì»¬ëŸ¼(cc.col_eq_status_filtered) í¬í•¨
    report : Dict[str, Any]
        - ì¸ë±ìŠ¤ ë¦¬í¬íŠ¸: 'idx_eq_status_filtered_nan', 'idx_eq_status_filtered_not_0', 'idx_eq_status_filtered_total'
        - ì¹´ìš´íŠ¸ ë“± ë¶€ê°€ ì •ë³´ ë¡œê·¸ì— ë‚¨ê¹€

    ì˜ˆì™¸
    ----
    - í•„ìˆ˜ ì»¬ëŸ¼ ì—†ìœ¼ë©´ KeyError ì¦‰ì‹œ ë°œìƒ
    """
    # -----------------------------------------
    # ë¡œê±° ì¤€ë¹„ (ì˜ˆ: ìƒìœ„ ì½”ë“œì—ì„œ get_loggerë¡œ ìƒì„±í•œ logger ì „ë‹¬)
    # -----------------------------------------
    # logger = get_logger(LoggerConfig(name="Preprocess", level=10))  # ìƒìœ„ì—ì„œ ìƒì„± ê°€ì •

    # ---------------------------------
    # ì‚¬ì „ ì¤€ë¹„: í”Œë˜ê·¸ ì»¬ëŸ¼ ë³´ì¥
    # ---------------------------------
    out = df.copy()
    
    col_flag = cc.col_eq_status_filtered
    if col_flag not in out.columns:
        out[col_flag] = False

    n = len(out)

    logger.info("â”Œâ”€[EQ] 1. EQ_Status ê¸°ë°˜ row filtering ì‹œì‘")
    
    # ---------------------------------
    # 0) í•„ìˆ˜ ì»¬ëŸ¼ ìœ íš¨ì„± ì²´í¬
    # ---------------------------------
    cols_tms_eq_status = list(cc.cols_tms_eq_status)
    cols_tms_value = list(cc.cols_tms_value)
    cols_icf_tms = list(cc.cols_icf_tms)

    missing = [c for c in (cols_tms_eq_status + cols_tms_value + cols_icf_tms) if c not in out.columns]
    if missing:
        raise KeyError(f"[EQ] í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing}")

    # ì„¼ì„œ â†’ íƒ€ê¹ƒ ì»¬ëŸ¼ ë§¤í•‘ (ê° EQ ìƒíƒœ ì»¬ëŸ¼ë§ˆë‹¤ ê°’/ICF ì»¬ëŸ¼)
    sensor_cols: Dict[str, List[str]] = {
        eq_col: [val_col, icf_col]
        for eq_col, val_col, icf_col in zip(cols_tms_eq_status, cols_tms_value, cols_icf_tms)
    }
    window_rows_eq_status_shift       = int(shift_sec         / resample_sec)
    window_rows_eq_status_min_nan_len = int(min_nan_block_sec / resample_sec)

    # ---------------------------------
    # A) ì¡°ê±´ 1~3 + ì‹œí”„íŠ¸: NaN ë§ˆìŠ¤í‚¹ + í”Œë˜ê·¸
    # ---------------------------------
    logger.info("â”‚  â”œâ”€[A] EQ_Status ì¡°ê±´(ê²°ì¸¡/!=0/==1 ì´í›„ shift) NaN ë§ˆìŠ¤í‚¹ ì‹œì‘")
    for col in cols_tms_eq_status:
        # ìƒíƒœ/íƒ€ê¹ƒ ê²€ì¦
        if col not in out.columns:
            raise KeyError(f"[EQ] EQ ìƒíƒœ ì»¬ëŸ¼ ëˆ„ë½: {col}")
        targets = sensor_cols.get(col, [])
        if not targets:
            raise KeyError(f"[EQ] íƒ€ê¹ƒ ì»¬ëŸ¼ ë§¤í•‘ ì—†ìŒ: {col}")
        missing_targets = [t for t in targets if t not in out.columns]
        if missing_targets:
            raise KeyError(f"[EQ] íƒ€ê¹ƒ ì»¬ëŸ¼ ëˆ„ë½: {missing_targets} (for {col})")

        status = out[col]
        cond_nan = status.isna()
        cond_non_zero = status.notna() & (status != 0)
        cond_1 = (status == 1)

        # ë³´ê³ ìš© ë¡œê·¸
        logger.info(
            "â”‚  â”‚  [%s] NaN:%d / !=0:%d / ==1:%d",
            col,
            int(cond_nan.sum()),
            int(cond_non_zero.sum()),
            int(cond_1.sum()),
        )

        # ìœ„ì¹˜ ê¸°ë°˜ ì‹œí”„íŠ¸(ì•ˆì „)
        pos_1 = np.flatnonzero(cond_1.to_numpy())
        post_mask = np.zeros(n, dtype=bool)
        if window_rows_eq_status_shift > 0 and pos_1.size > 0:
            ends = np.minimum(pos_1 + window_rows_eq_status_shift, n - 1)
            for s, e in zip(pos_1, ends):
                post_mask[s:e + 1] = True

        full_mask = cond_nan.to_numpy() | cond_non_zero.to_numpy() | post_mask
        logger.info("â”‚  â”‚   ì´í›„ shift í¬í•¨ ì´ ë§ˆìŠ¤í‚¹ ìˆ˜: %d", int(full_mask.sum()))

        # ë§ˆìŠ¤í‚¹ ì „/í›„ ë¹„êµë¡œ "ìƒˆë¡œ NaNëœ í–‰" â†’ í”Œë˜ê·¸ True
        before_isna = out[targets].isna()
        out.loc[full_mask, targets] = np.nan
        after_isna = out[targets].isna()
        newly_masked_rows = (after_isna & ~before_isna).any(axis=1)

        # í”Œë˜ê·¸ ë°˜ì˜
        out.loc[newly_masked_rows.index[newly_masked_rows], col_flag] = True

    # ---------------------------------
    # B) ì—°ì† NaN â‰¥ ì„ê³„ê¸¸ì´: í”Œë˜ê·¸ë§Œ True (ê°’ ë³€ê²½ ì—†ìŒ)
    # ---------------------------------
    logger.info("â”‚  â”œâ”€[B] ì—°ì† NaN â‰¥ %dí–‰ í”Œë˜ê·¸ ì²˜ë¦¬ ì‹œì‘", int(window_rows_eq_status_min_nan_len))
    all_sensor_cols = sum(sensor_cols.values(), [])
    missing_targets_all = [t for t in all_sensor_cols if t not in out.columns]
    if missing_targets_all:
        raise KeyError(f"[EQ] íƒ€ê¹ƒ ì»¬ëŸ¼ ëˆ„ë½: {missing_targets_all} (ì—°ì† NaN íŒì •ìš©)")

    nan_all_mask = out[all_sensor_cols].isna().all(axis=1)
    group = (nan_all_mask != nan_all_mask.shift()).cumsum()
    run_sizes = nan_all_mask.groupby(group).transform('size')
    long_nan = nan_all_mask & (run_sizes >= int(window_rows_eq_status_min_nan_len))

    # ê°’ ë³€ê²½ ì—†ì´ í”Œë˜ê·¸ë§Œ
    out.loc[long_nan, col_flag] = True

    idx_eq_status_filtered_nan = out.index[long_nan]
    logger.info("â”‚  â”‚  ì—°ì† NaN(ì„ê³„ê¸¸ì´â†‘) ìˆ˜: %d", int(long_nan.sum()))

    # ---------------------------------
    # C) ëª¨ë“  EQ==0ë§Œ í—ˆìš©: í”Œë˜ê·¸ë§Œ True (ê°’ ë³€ê²½ ì—†ìŒ)
    # ---------------------------------
    logger.info("â”‚  â””â”€[C] ëª¨ë“  EQ_Status == 0 ê°•ì œ í”Œë˜ê·¸ ì²˜ë¦¬ ì‹œì‘")
    missing_eq_cols = [c for c in cols_tms_eq_status if c not in out.columns]
    if missing_eq_cols:
        raise KeyError(f"[EQ] EQ ìƒíƒœ ì»¬ëŸ¼ ëˆ„ë½: {missing_eq_cols}")

    eq_mask = (out[cols_tms_eq_status] == 0).all(axis=1)
    not_all_zero = ~eq_mask
    out.loc[not_all_zero, col_flag] = True

    idx_eq_status_filtered_not_0 = out.index[not_all_zero]
    logger.info("â”‚     EQ!=0 í¬í•¨ ìˆ˜: %d", int(not_all_zero.sum()))

    # ---------------------------------
    # ìµœì¢… ìš”ì•½
    # ---------------------------------
    idx_eq_status_filtered_total = idx_eq_status_filtered_nan.union(idx_eq_status_filtered_not_0)

    logger.info("â”œâ”€[EQ] ìš”ì•½")
    logger.info("â”‚  (ì°¸ê³ ) shift í–‰ìˆ˜: %d", int(window_rows_eq_status_shift))
    logger.info("â”‚  (ì°¸ê³ ) ì—°ì† NaN ì„ê³„ í–‰ìˆ˜: %d", int(window_rows_eq_status_min_nan_len))
    logger.info("â”‚  (1) ì—°ì† NaN: %d", len(idx_eq_status_filtered_nan))
    logger.info("â”‚  (2) EQ!=0 : %d", len(idx_eq_status_filtered_not_0))
    logger.info("â”‚  (1)+(2)  : %d", len(idx_eq_status_filtered_total))
    logger.info("â””â”€[EQ] ì™„ë£Œ")
    return out

# ============================================================
# INCINERATOR_STATUS: Â±window í”Œë˜ê·¸ë§Œ ì„¸íŒ… (nanìœ¼ë¡œ ê°’ ë³€ê²½ ì—†ìŒ)
# ============================================================
def _apply_inc_status_mask(
    df: pd.DataFrame,
    *,
    col_datetime: str,
    col_inc_status: str,
    abnormal_value: int,
    resample_sec: int,
    window_min: int,
    logger: logging.Logger,
    col_flag: str,  # ColumnConfig.col_inc_status_filtered
) -> pd.DataFrame:
    """
    INCINERATOR_STATUSê°€ abnormal_valueì¸ ì§€ì ë“¤ì„ ì¤‘ì‹¬ìœ¼ë¡œ
    Â±(window_min ë¶„) â‰’ Â±window_rows í–‰ ë²”ìœ„ë¥¼ 'í–‰ ê¸°ì¤€'ìœ¼ë¡œ ê³„ì‚°.
    - ê°’ì€ ë³€ê²½í•˜ì§€ ì•ŠìŒ
    - í•´ë‹¹ êµ¬ê°„(íŒ½ì°½ êµ¬ê°„)ì— ëŒ€í•´ col_flag=True ì„¸íŒ…ë§Œ ìˆ˜í–‰

    Notes
    -----
    - ì‹œê°„ ê¸°ë°˜ ë³‘í•©(íƒ€ì„ìŠ¤íƒ¬í”„ ë¹„êµ) ì‚¬ìš©í•˜ì§€ ì•ŠìŒ.
    - ì»¬ëŸ¼ ëˆ„ë½ ì‹œ KeyError.
    """
    out = df.copy()
    out[col_datetime] = pd.to_datetime(out[col_datetime])
    out = out.sort_values(col_datetime).reset_index(drop=True)
    n = len(out)

    # í”Œë˜ê·¸ ì»¬ëŸ¼ ë³´ì¥
    if col_flag not in out.columns:
        out[col_flag] = False

    # í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì¦
    if n == 0:
        logger.info("[INC] ë¹ˆ DataFrame â†’ ìŠ¤í‚µ")
        return out
    if col_inc_status not in out.columns:
        raise KeyError(f"[INC] ìƒíƒœ ì»¬ëŸ¼ ëˆ„ë½: {col_inc_status}")
    if col_datetime not in out.columns:
        raise KeyError(f"[INC] ì‹œê°„ ì»¬ëŸ¼ ëˆ„ë½: {col_datetime}")

    # ìƒíƒœ ë§ˆìŠ¤í¬
    mask_bad = (out[col_inc_status] == abnormal_value)
    if not mask_bad.any():
        logger.info("[INC] abnormal=%s í–‰ ì—†ìŒ â†’ ìŠ¤í‚µ", abnormal_value)
        return out

    # ----- í–‰ ê¸°ì¤€ ìœˆë„ìš° í¬ê¸° ê³„ì‚° -----
    if resample_sec is None or resample_sec <= 0:
        raise ValueError(f"[INC] resample_secê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {resample_sec}")
    window_rows = int(round((window_min * 60) / resample_sec))
    window_rows = max(0, window_rows)

    logger.info("[INC] abnormal=%s, window_min=%d, resample_sec=%d â†’ Â±%dí–‰(ì´ %dí–‰ ì»¤ë„)",
                abnormal_value, int(window_min), int(resample_sec), window_rows, 2 * window_rows + 1)

    # ----- í–‰ ê¸°ì¤€ íŒ½ì°½ ë§ˆìŠ¤í¬ (convolution) -----
    v = mask_bad.to_numpy(dtype=int)
    if window_rows == 0:
        window_mask = v.astype(bool)
    else:
        kernel = np.ones(2 * window_rows + 1, dtype=int)
        window_mask = np.convolve(v, kernel, mode='same') > 0

    # ----- ê°’ ë³€ê²½ ì—†ì´ í”Œë˜ê·¸ë§Œ ì„¸íŒ… -----
    prev_true = out[col_flag].sum()
    out.loc[window_mask, col_flag] = True
    flagged = int(out[col_flag].sum() - prev_true)

    logger.info("[INC] í”Œë˜ê·¸ ì„¸íŒ… í–‰ìˆ˜=%d (íŒ½ì°½ ë§ˆìŠ¤í¬ ì´í•©=%d)", flagged, int(window_mask.sum()))
    return out

# ============================================================
# ê¸€ë¡œë²Œ ì„ê³„ê°’: í”Œë˜ê·¸ë§Œ ì„¸íŒ… (nanìœ¼ë¡œ ê°’ ë³€ê²½ ì—†ìŒ)
# ============================================================
def _apply_global_threshold_mask(
    df: pd.DataFrame,
    *,
    thresholds: Dict[str, Tuple[float, float]],
    numeric_cols: Sequence[str],
    col_flag: str,  # ColumnConfig.col_glob_threshold_filtered
) -> pd.DataFrame:
    """
    thresholdsì— ë”°ë¼ numeric_colsì—ì„œ ì„ê³„ë²”ìœ„ ë°–ì˜ ê°’ì´ ìˆëŠ” í–‰ì„ íƒì§€.
    - ê°’ì€ ë³€ê²½í•˜ì§€ ì•ŠìŒ
    - í•´ë‹¹ í–‰ì— ëŒ€í•´ col_flag=True ì„¸íŒ…ë§Œ ìˆ˜í–‰
    """
    out = df.copy()
    if len(out) == 0:
        if col_flag not in out.columns:
            out[col_flag] = False
        return out

    if col_flag not in out.columns:
        out[col_flag] = False

    # ìœ íš¨ numeric ì»¬ëŸ¼ë§Œ ì‚¬ìš©
    valid_cols = [c for c in numeric_cols if c in out.columns and pd.api.types.is_numeric_dtype(out[c])]
    if not valid_cols:
        return out

    # ì»¬ëŸ¼ë³„ ì„ê³„ ë°– ì—¬ë¶€ ê³„ì‚° â†’ í–‰ ë‹¨ìœ„ í•©ì§‘í•©
    flag_mask = pd.Series(False, index=out.index)
    for col in valid_cols:
        lo, hi = thresholds.get(col, (0.0, np.inf))
        s = out[col]
        # "ë§ˆìŠ¤í‚¹ ëŒ€ìƒ"ì€ ì •ìƒë²”ìœ„ ë°–ì´ë©´ì„œ ê°’ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°ë¡œ ì •ì˜
        out_of_range = s.notna() & ~s.between(lo, hi)
        flag_mask |= out_of_range

    out.loc[flag_mask, col_flag] = True
    return out

    
    

# ============================================================
# Public API
# ============================================================
@dataclass
class Preprocessor:
    """
    Config-driven ì „ì²˜ë¦¬ê¸°.

    Methods
    -------
    make_test_ffill(df, cfg, require_full_index=False, freq=None)
        - ì œí•œ ffill ë°ì´í„°ì…‹ ìƒì„± (ì¶”ì²œê°’ ê³„ì‚°ìš©)
    make_train_dataset(df, cfg)
        - í•™ìŠµìš© ë°ì´í„°ì…‹ ìƒì„±:
          ì‹œê°„í•„í„° â†’ ë¹„ì •ìƒì œì™¸ â†’ ì´ìƒì¹˜â†’NaN â†’ ì‹œê°„ì œì•½ë³´ê°„
    """
    # ê³µí†µ ìŠ¤í‚¤ë§ˆ/ë£°
    column_config: ColumnConfig = field(default_factory=ColumnConfig)

    # ì „ì²˜ë¦¬ ì„¤ì •(í•™ìŠµ/ì¶”ë¡ )
    model_config: GPModelConfig = field(default_factory=GPModelConfig)
    prep_cm_train_cfg: TrainPreprocessingConfig = field(default_factory=TrainPreprocessingConfig)
    prep_gp_train_cfg: GPTrainPreprocessingConfig = field(default_factory=GPTrainPreprocessingConfig)
    prep_infer_cfg: InferPreprocessingConfig = field(default_factory=InferPreprocessingConfig)

    # -----------------------------
    # [TEST] ì¶”ì²œê°’ ê³„ì‚°ìš© ffill
    # -----------------------------
    def make_infer_ffill(
        self,
        df: pd.DataFrame,
        *,
        require_full_index: bool = True,
        logger_cfg: Optional[LoggerConfig] = None,
    ) -> pd.DataFrame:
        """
        ì¶”ì²œ ê³„ì‚°ìš© í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„±:
        - ì£¼ì–´ì§„ raw ë°ì´í„°í”„ë ˆì„ì— ëŒ€í•´ ì œí•œëœ ffillì„ ìˆ˜í–‰í•˜ì—¬ ì¶”ë¡  ì‹œ ì…ë ¥ ë°ì´í„° í’ˆì§ˆì„ ë³´ì¥í•©ë‹ˆë‹¤.

        ì²˜ë¦¬ ìˆœì„œ
        ----------
        1) datetime ì»¬ëŸ¼ì„ pandas datetimeìœ¼ë¡œ ë³€í™˜ ë° ì •ë ¬
        2) (ì˜µì…˜) full index ìƒì„±: ì‹œê°„ì¶•ì´ ê· ì¼í•˜ì§€ ì•Šì€ ê²½ìš° full indexë¥¼ ìƒì„±í•˜ì—¬ ê²°ì¸¡ ì‹œì  í¬í•¨
        3) ì œí•œ ffill ìˆ˜í–‰: config.ffill_limit_sec ì´í•˜ì˜ ê²°ì¸¡ë§Œ forward-fillë¡œ ì±„ì›€

        Parameters
        ----------
        df : pd.DataFrame
            ì¶”ì²œ ê³„ì‚°ìš© ì›ë³¸ ë°ì´í„°í”„ë ˆì„(ìµœê·¼ êµ¬ê°„ ë°ì´í„°)
        require_full_index : bool, default True
            True â†’ full index ìƒì„± í›„ ffill ìˆ˜í–‰
            False â†’ ì…ë ¥ ë°ì´í„°ê°€ ì´ë¯¸ ê· ì¼ ê°„ê²©ì´ë¼ê³  ê°€ì •í•˜ê³  ffillë§Œ ìˆ˜í–‰
        freq : str, optional
            full index ê°„ê²©(ì˜ˆ: '5s'). ë¯¸ì§€ì • ì‹œ cfg.resample_sec ì‚¬ìš©
        logger_cfg : LoggerConfig, optional
            ë³„ë„ì˜ ë¡œê¹… ì„¤ì •ì„ ì ìš©í•  ê²½ìš° ì§€ì •

        Returns
        -------
        pd.DataFrame
            ì œí•œ ffillì´ ì ìš©ëœ í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°í”„ë ˆì„
            - col_datetime ê¸°ì¤€ ì •ë ¬ ì™„ë£Œ
            - (ì˜µì…˜) full index ì ìš©
            - ì œí•œ ì¡°ê±´ì— ë”°ë¼ NaN ì¼ë¶€ ì”ì¡´ ê°€ëŠ¥
        """
        ic = self.prep_infer_cfg
        tc = self.prep_infer_cfg
        cc = self.column_config
        # ë¡œê±° ìš°ì„ ìˆœìœ„: ì¸ì logger_cfg > cfg.logger_cfg > ê¸°ë³¸ê°’
        lg = get_logger(logger_cfg or getattr(ic, "logger_cfg", LoggerConfig(name="Preprocess.TEST", level=10)))

        col_datetime = cc.col_datetime
        col_real_row = cc.col_real_row
        out = df.copy()
        if out.empty:
            lg.warning("[TEST] ì…ë ¥ dfê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ê·¸ëŒ€ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.")
            return out

        # 1) datetime ë³´ì¥ ë° ì •ë ¬
        out[col_datetime] = _datetime_series(out, col_datetime)
        out = out.sort_values(col_datetime).reset_index(drop=True)
        lg.debug(f"[TEST] ì…ë ¥ í–‰ìˆ˜: {len(out):,}")

        # 2) full index (ì˜µì…˜)
        f = _freq_from_seconds(tc.resample_sec)
        if require_full_index:
            lg.info(f"[TEST] full index ìƒì„±: freq='{f}'")
            out = _ensure_full_index(out, col_datetime, freq=f, col_real_row=col_real_row)
            lg.debug(f"[TEST] full index í›„ í–‰ìˆ˜: {len(out):,}")

        # 3) ì œí•œ ffill
        lg.info(f"[TEST] ì œí•œ ffill: limit_sec={ic.ffill_limit_sec}, freq='{f}'")
        out = _limited_ffill(out, col_datetime, f, ic.ffill_limit_sec)

        lg.info("[TEST] ffill ì™„ë£Œ")
        return out

    # -----------------------------
    # [TRAIN] í•™ìŠµ ë°ì´í„° ìƒì„± (ê³µí†µ ì „ì²˜ë¦¬)
    # -----------------------------
    def make_train_dataset(
        self,
        df: pd.DataFrame,
        *,
        logger_cfg: Optional[LoggerConfig] = None,
    ) -> pd.DataFrame:
        """
        í•™ìŠµìš© ë°ì´í„°ì…‹ ìƒì„± íŒŒì´í”„ë¼ì¸:
        - config ê¸°ë°˜ìœ¼ë¡œ ì´ìƒì¹˜ ì œê±°, ê²°ì¸¡ ë³´ê°„, ë¹„ì •ìƒ ìƒíƒœ êµ¬ê°„ ì œì™¸ ë“±ì„ ìˆ˜í–‰í•˜ì—¬
          ëª¨ë¸ í•™ìŠµì— ì í•©í•œ ë°ì´í„°ì…‹ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

        ì²˜ë¦¬ ìˆœì„œ
        ----------
        (ì„ í–‰) ì‹œê°„ í•„í„°: start_time ~ end_time ë²”ìœ„ë¡œ ë°ì´í„° ì œí•œ -> í˜„ì¬ ì•ˆ í•¨
        # ì¶”í›„ S3 ì—ì„œ ê°€ì ¸ì˜¬ ë•Œë¶€í„° ì‹œê°„ìœ¼ë¡œ í•„í„°ë§ í•´ì„œ ê°€ì ¸ì˜¤ëŠ” ê±¸ ê°€ì •í•˜ê³  ì‘ì„± ============
        st = getattr(cfg, "start_time", None)
        et = getattr(cfg, "end_time", None)
        out[col_datetime] = _datetime_series(out, col_datetime)
        out = out.sort_values(col_datetime).reset_index(drop=True)
        if st or et:
            lg.info(f"[TRAIN-COMMON] ì‹œê°„ í•„í„° ì ìš©: start={st}, end={et}")
            if st: out = out.loc[out[col_datetime] >= pd.to_datetime(st)]
            if et: out = out.loc[out[col_datetime] <= pd.to_datetime(et)]
            out = out.reset_index(drop=True)
            lg.debug(f"[TRAIN-COMMON] ì‹œê°„ í•„í„° í›„ í–‰ìˆ˜: {len(out):,}")
        if out.empty:
            lg.error("[TRAIN-COMMON] ì‹œê°„ í•„í„° ê²°ê³¼ê°€ ë¹„ì—ˆìŠµë‹ˆë‹¤.")
            return out
        # ì¶”í›„ S3 ì—ì„œ ê°€ì ¸ì˜¬ ë•Œë¶€í„° ì‹œê°„ìœ¼ë¡œ í•„í„°ë§ í•´ì„œ ê°€ì ¸ì˜¤ëŠ” ê±¸ ê°€ì •í•˜ê³  ì‘ì„± ============
        0) ì‹œê°„ reindexing (full index) ìˆ˜í–‰
        1) EQ_Status ê¸°ë°˜ row filtering
        2) ì†Œê°ë¡œ ë¹„ì •ìƒ ê°€ë™ êµ¬ê°„ ì œì™¸
        3) ì„ê³„ê°’ ê¸°ë°˜ ì´ìƒì¹˜ ì²˜ë¦¬: out-of-range â†’ NaN
        4) ì‹œê°„ ì œì•½ ë³´ê°„: ì•/ë’¤ ìœ íš¨ ê´€ì¸¡ì´ limit_sec ì´ë‚´ì¸ ê²½ìš°ë§Œ ë³´ê°„
        5) ì‹œê°„ reindexingìœ¼ë¡œ ìƒê¸´ rows ì œì™¸(col_real_row == Trueë§Œ ìœ ì§€)
        
        Parameters
        ----------
        df : pd.DataFrame
            ì›ë³¸ ë°ì´í„°í”„ë ˆì„
        logger_cfg : LoggerConfig, optional
            ë³„ë„ì˜ ë¡œê¹… ì„¤ì •ì„ ì ìš©í•  ê²½ìš° ì§€ì •

        Returns
        -------
        pd.DataFrame
            í•™ìŠµì— ì í•©í•˜ë„ë¡ ì „ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„.
            - ì‹œê°„ í•„í„°, ë¹„ì •ìƒ êµ¬ê°„ ì œì™¸, ì„ê³„ê°’ ê¸°ë°˜ NaN ì²˜ë¦¬, ì‹œê°„ ì œì•½ ì„ í˜•ë³´ê°„ í›„ ìµœì¢… ë°ì´í„° ë°˜í™˜
        """
        tc = self.prep_cm_train_cfg
        cc = self.column_config
        mc = self.model_config
        lg = get_logger(logger_cfg or getattr(tc, "logger_cfg", LoggerConfig(name="Preprocess.TRAIN", level=10)))

        col_datetime = cc.col_datetime
        col_real_row = cc.col_real_row
        out = df.copy()
        if out.empty:
            lg.warning("[TRAIN-COMMON] ì…ë ¥ dfê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ê·¸ëŒ€ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.")
            return out

        # í•„ìš”í•œ ê²½ìš° ì‹œê°„ í•„í„°ëŠ” ìƒìœ„ I/Oì—ì„œ ì²˜ë¦¬
        out[col_datetime] = _datetime_series(out, col_datetime)
        out = out.sort_values(col_datetime).reset_index(drop=True)

        # 0) ì‹œê°„ reindexing (full index)
        f = _freq_from_seconds(int(tc.resample_sec))
        before_n = len(out)
        out = _ensure_full_index(out, col_datetime, freq=f, col_real_row=col_real_row)
        after_n = len(out)
        added_n = after_n - before_n
        lg.info("[TRAIN-COMMON][FULL-INDEX] í–‰ìˆ˜: %s â†’ %s (ì¶”ê°€ %s)", f"{before_n:,}", f"{after_n:,}", f"{added_n:,}")

        # 1) EQ_Status ê¸°ë°˜ row filtering (col_real_rowì™€ ë¬´ê´€)
        out = _apply_eq_status_mask(
            df=out,
            col_datetime=col_datetime,
            sensor_cols=cc.eq_map,                    # â† eq_map ì‚¬ìš©
            resample_sec=int(tc.resample_sec),
            shift_sec=int(tc.eq_shift_sec),           # â† eq_shift_sec ì‚¬ìš©
            min_nan_block_sec=int(tc.eq_min_nan_block_sec),  # â† eq_min_nan_block_sec
            logger=lg,
            cc=cc,
            col_flag=cc.col_eq_status_filtered,
        )
        
        # 2) ì†Œê°ë¡œ ë¹„ì •ìƒ ê°€ë™ êµ¬ê°„ ì œì™¸ (col_real_rowì™€ ë¬´ê´€)
        col_inc_status = cc.col_inc_status
        before = len(out)
        out = _apply_inc_status_mask(
            df=out,
            col_datetime=col_datetime,
            col_inc_status=col_inc_status,
            abnormal_value=tc.exclude_status_value,
            resample_sec=int(tc.resample_sec),
            window_min=tc.exclude_window_min,
            logger=lg,
            col_flag=cc.col_inc_status_filtered,
        )
        lg.info("[TRAIN-COMMON] ë¹„ì •ìƒ ìƒíƒœ: %s == %s", col_inc_status, tc.exclude_status_value)
        lg.info("[TRAIN-COMMON] ë¹„ì •ìƒ ë²”ìœ„: ë¹„ì •ìƒ ìƒíƒœ rows + ì•ë’¤ %dë¶„", tc.exclude_window_min)
        lg.info("[TRAIN-COMMON] ë¹„ì •ìƒ ë²”ìœ„ ì œì™¸: %s â†’ %s", f"{before:,}", f"{len(out):,}")
        if out.empty:
            lg.error("[TRAIN-COMMON] ë¹„ì •ìƒ ì œì™¸ í›„ ë°ì´í„°ê°€ ë¹„ì—ˆìŠµë‹ˆë‹¤.")
            return out

        # 3) ì„ê³„ê°’ ë°– NaN
        out = _apply_global_threshold_mask(
            df=out,
            thresholds=tc.global_threshold,
            numeric_cols=tc._numeric_var_keys,
            col_flag=cc.col_glob_threshold_filtered,
        )
    
        lg.info("[TRAIN-COMMON] ì„ê³„ê°’ ê¸°ë°˜ ì´ìƒì¹˜ ë§ˆìŠ¤í‚¹ ì™„ë£Œ")

        # 4) ì‹œê°„ ì œì•½ ë³´ê°„
        out = _time_limited_interpolate(
            df=out,
            col_datetime=col_datetime,
            target_cols=mc._training_required(),
            limit_sec=int(tc.interpolate_limit_sec),
            method=str(tc.interpolate_method),
        )
        lg.info("[TRAIN-COMMON] ì‹œê°„ ì œì•½ ë³´ê°„ ì™„ë£Œ: method='%s', limit_sec=%s",
                tc.interpolate_method, tc.interpolate_limit_sec)

        # === [FLAGS FILTER] ë³´ê°„ í›„: í”Œë˜ê·¸ 3ì¢…ì˜ í•©ì§‘í•©(Trueê°€ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ ì œì™¸)ë§Œë“¤ì–´ í•„í„° ===
        flag_cols = [cc.col_eq_status_filtered, cc.col_inc_status_filtered, cc.col_glob_threshold_filtered]
        
        # í•„ìˆ˜ í”Œë˜ê·¸ ì»¬ëŸ¼ ì¡´ì¬ ê²€ì¦ (ì—†ìœ¼ë©´ ì¦‰ì‹œ ì—ëŸ¬)
        missing_flags = [c for c in flag_cols if c not in out.columns]
        if missing_flags:
            raise KeyError(f"[TRAIN-COMMON][FLAGS] í”Œë˜ê·¸ ì»¬ëŸ¼ ëˆ„ë½: {missing_flags}")
        
        # ì•ˆì „: bool ë³€í™˜ ë° ê²°ì¸¡ False ì²˜ë¦¬
        flags_df = out[flag_cols].astype(bool).fillna(False)
        
        # ë¡œê·¸ë¥¼ ìœ„í•´ í•„í„° ì „ ì§‘ê³„
        before_flags = len(out)
        flag_counts = {c: int(flags_df[c].sum()) for c in flag_cols}
        any_flag_true = flags_df.any(axis=1)
        n_any_true = int(any_flag_true.sum())
        
        # all False (== ì–´ë–¤ í•„í„°ì—ë„ ê±¸ë¦¬ì§€ ì•ŠìŒ) í–‰ë§Œ ìœ ì§€
        keep_mask = ~any_flag_true
        out = out.loc[keep_mask].copy()
        after_flags = len(out)
        
        lg.info(
            "[TRAIN-COMMON][FLAGS] í•©ì§‘í•© ì œì™¸ ì ìš©: %s â†’ %s (ì œê±° %s)",
            f"{before_flags:,}", f"{after_flags:,}", f"{(before_flags - after_flags):,}"
        )
        lg.info(
            "[TRAIN-COMMON][FLAGS] ìƒì„¸: any(True)=%s, %s=%d, %s=%d, %s=%d",
            f"{n_any_true:,}",
            cc.col_eq_status_filtered, flag_counts[cc.col_eq_status_filtered],
            cc.col_inc_status_filtered, flag_counts[cc.col_inc_status_filtered],
            cc.col_glob_threshold_filtered, flag_counts[cc.col_glob_threshold_filtered],
        )

        # 5) ì‹œê°„ reindexingìœ¼ë¡œ ìƒê¸´ rows ì œì™¸ (real rowë§Œ ìœ ì§€)
        before_real_filter = len(out)
        if col_real_row not in out.columns:
            raise ValueError(f"[TRAIN-COMMON] '{col_real_row}' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. full index ìƒì„± ë‹¨ê³„ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        out = out.loc[out[col_real_row].astype(bool)].copy()
        after_real_filter = len(out)
        removed_fake = before_real_filter - after_real_filter
        lg.info("[TRAIN-COMMON][REAL-ONLY] ì‚½ì…í–‰ ì œê±°: %s â†’ %s (ì œê±° %s)",
                f"{before_real_filter:,}", f"{after_real_filter:,}", f"{removed_fake:,}")
    
        lg.info("[TRAIN-COMMON] ê³µí†µ ì „ì²˜ë¦¬ ì™„ë£Œ (ìµœì¢… í–‰ìˆ˜: %s)", f"{len(out):,}")
        return out
        
        
    # -----------------------------
    # [TRAIN] GP ì „ìš© ì¶”ê°€ ì „ì²˜ë¦¬: AIìš´ì „ì—¬ë¶€ í•„í„°ë§ â†’ ì¤‘ë³µí–‰ ì œê±° â†’ ìƒ˜í”Œë§ â†’ ê²°ì¸¡ì¹˜ ì œê±° â†’ ìµœì†Œ ìœ íš¨ìƒ˜í”Œ ê°œìˆ˜ ì²´í¬
    # ----------------------------- 
    def make_train_gp(
        self,
        df: pd.DataFrame,
        *,
        logger_cfg: Optional[LoggerConfig] = None,
    ) -> pd.DataFrame:
        """
        GP ëª¨ë¸ í•™ìŠµ ì „ì²˜ë¦¬(ê³µí†µ + GP ì¶”ê°€ ë‹¨ê³„):
        - ê³µí†µ ì „ì²˜ë¦¬(make_train_common) ìˆ˜í–‰ í›„,
        - 1) AI ìš´ì „ ì—¬ë¶€ í•„í„°ë§: ColumnConfig.col_ai == 1 ì¸ í–‰ë§Œ ë‚¨ê¹€
        - 2) ì¤‘ë³µí–‰ ì œê±°: dedup (subset=í•µì‹¬ í”¼ì²˜)
        - 3) ìƒ˜í”Œë§: sample (n=gc.sample_size, rs=gc.random_state)
        - 4) ê²°ì¸¡ì¹˜ ì œê±°: dropna (subset=í•µì‹¬ í”¼ì²˜, gc.dropna_required)
        - 5) ìµœì†Œ ìœ íš¨ìƒ˜í”Œ ê°œìˆ˜ ì²´í¬: min_samples ê²½ê³ 
        """
        gc = self.prep_gp_train_cfg
        tc = self.prep_cm_train_cfg
        cc = self.column_config
        mc = self.model_config
        lg = get_logger(
            logger_cfg or getattr(gc, "logger_cfg", LoggerConfig(name="Preprocess.GP", level=10))
        )
        col_datetime = cc.col_datetime

        # ------------------------------------------------------------------
        # 0) (ì¤‘ìš”) ê³µí†µ ì „ì²˜ë¦¬ ì œê±°
        #    ì…ë ¥ dfëŠ” ì´ë¯¸ ê³µí†µ ì „ì²˜ë¦¬(make_train_dataset) ì™„ë£Œë³¸ì´ì–´ì•¼ í•¨
        # ------------------------------------------------------------------
        out = df.copy()
        lg.info("[TRAIN-GP] GP ì „ìš© ì „ì²˜ë¦¬ ì‹œì‘ (ì…ë ¥=ê³µí†µ ì „ì²˜ë¦¬ ì™„ë£Œë³¸ ê°€ì •)")
        lg.debug("[TRAIN-GP] ì…ë ¥ í¬ê¸°=%s", out.shape)

        # (ì•ˆì „) datetime ì •ë ¬ ë³´ì¥
        if col_datetime in out.columns:
            out[col_datetime] = _datetime_series(out, col_datetime)
            out = out.sort_values(col_datetime).reset_index(drop=True)

        # ------------------------------------------------------------------
        # 1) AI ìš´ì „ í•„í„° (í•„ìˆ˜ ë‹¨ê³„)
        #    - ë°˜ë“œì‹œ ColumnConfig.col_ai ê°€ ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•˜ë©°,
        #      ì…ë ¥ df ì— í•´ë‹¹ ì»¬ëŸ¼ì´ ì¡´ì¬í•´ì•¼ í•©ë‹ˆë‹¤.
        #    - ì¡°ê±´: df[cc.col_ai] == 1 ì¸ í–‰ë§Œ ìœ ì§€
        #    - ë¯¸ì¡´ì¬ ì‹œ, ëª…ì‹œì  ì—ëŸ¬ë¡œ ì¤‘ë‹¨(ì„¤ì •/ìŠ¤í‚¤ë§ˆ ë³´ì™„ ìœ ë„)
        # ------------------------------------------------------------------
        if not hasattr(cc, "col_ai") or cc.col_ai not in out.columns:
            raise ValueError("[TRAIN-GP] ColumnConfig.col_ai ëˆ„ë½ ë˜ëŠ” ì…ë ¥ ë°ì´í„°ì— ì—†ìŒ.")
        before_ai = len(out)
        out = out.loc[out[cc.col_ai] == 1].copy()
        lg.info("[TRAIN-GP] AI ìš´ì „ í•„í„° ì ìš©: %d â†’ %d (ì¡°ê±´: %s == 1)", before_ai, len(out), cc.col_ai)
        if out.empty:
            raise ValueError("[TRAIN-GP] AI ìš´ì „(=1) êµ¬ê°„ ì—†ìŒ.")

        # ------------------------------------------------------------------
        # 2) dedup / 3) sample / 4) dropna / 5) min_samples
        #    - í•µì‹¬ í”¼ì²˜(subset) = ColumnConfig ê¸°ì¤€ í•„ìˆ˜ í”¼ì²˜ ì§‘í•©(ì¡´ì¬ ì»¬ëŸ¼ë§Œ)
        # ------------------------------------------------------------------
        req_cols = mc._training_required()
        # # ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ í•„í„°
        # req_cols = [c for c in req_cols if c in out.columns]
        lg.debug("[TRAIN-GP] í•„ìˆ˜ ì»¬ëŸ¼=%s", req_cols)
        missing = [c for c in req_cols if c not in out.columns]
        if missing:
            msg = f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing} (ìš”êµ¬: {req_cols})"
            lg.error("[TRAIN-GP] %s", msg)
            raise ValueError(msg)

        # 2) ì¤‘ë³µí–‰ ì œê±°: dedup
        if gc.dedup and req_cols:
            before = len(out)
            out = out.drop_duplicates(subset=req_cols, keep="first")
            lg.info(f"[TRAIN-GP] ì¤‘ë³µ ì œê±°(subset={req_cols}): {before:,} â†’ {len(out):,} í–‰")

        # 3) ìƒ˜í”Œë§: sample
        if gc.sample_size is not None and len(out) > int(gc.sample_size):
            before = len(out)
            rs = int(getattr(gc, "random_state", 42))
            out = out.sample(n=int(gc.sample_size), random_state=rs).sort_values(col_datetime).reset_index(drop=True)
            lg.info(f"[TRAIN-GP] ìƒ˜í”Œë§: {before:,} â†’ {len(out):,} (n={gc.sample_size}, rs={gc.random_state})")

        # 4) ê²°ì¸¡ì¹˜ ì œê±°: dropna
        if gc.dropna_required and req_cols:
            before = len(out)
            out = out.dropna(subset=req_cols)
            lg.info(f"[TRAIN-GP] ê²°ì¸¡ í–‰ ì œê±°(subset={req_cols}): {before:,} â†’ {len(out):,} í–‰")

        # 5) ìµœì†Œ ìƒ˜í”Œ í™•ë³´ í™•ì¸: min_samples
        if len(out) < int(gc.min_samples):
            lg.warning(f"[TRAIN-GP] min_samples ë¯¸ë§Œ: {len(out)} < {gc.min_samples}.")
            lg.warning(f"í•™ìŠµì— ë¶€ì í•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        lg.info("[TRAIN-GP] GP ì „ìš© ì „ì²˜ë¦¬ ì™„ë£Œ")
        return out
        
        
        
"""
LightGBM ì¶”ê°€

ê³ ë¯¼, 2025-09-08
- train ì™¸ inferìš© preprocessing í•„ìš”í• ì§€? (`act_status` ë“±)
- GPëŠ” class ì•ˆì— methodë¡œ ì²˜ë¦¬í•˜ëŠ”ë°, LGBMì€ class ë³„ë„ë¡œ ìƒì„±
- logger ì¶”ê°€ í•„ìš”
    - (As-Is) print
    - (To-Do) logger
"""

# === Local application ===
from config.preprocessing_config import _LGBMWindowsMixin, LGBMTrainPreprocessingConfig, LGBMInferPreprocessingConfig

# === Helper í•¨ìˆ˜ ===

def generate_interval_summary_features_time(
    df: pd.DataFrame,
    *,
    datetime_col: str,
    columns: Sequence[str],
    windows_summary_sec: Sequence[int],
    windows_rate_sec: Sequence[int],
    coerce_numeric: bool = True,
    return_df: bool = True,
    rolling_closed: str | None = None,
    rolling_min_periods: int | None = None,
):
    df_work = df.sort_values(datetime_col).copy()
    t = pd.to_datetime(df_work[datetime_col].values)

    # ì›ë³¸ê³¼ ë™ë“±í•œ Î”t ê³„ì‚°
    dt_sec = pd.Series(t, index=t).diff().dt.total_seconds().replace(0, np.nan)

    new_columns: list[str] = []

    for col in columns:
        s_raw = pd.to_numeric(df_work[col], errors="coerce") if coerce_numeric else df_work[col]
        s_val = pd.Series(s_raw.to_numpy(), index=t)

        rate_per_sec = s_val.diff() / dt_sec

        out_chunks: list[pd.DataFrame] = []

        # (1) mean/std
        for sec in (windows_summary_sec or []):
            win = f"{sec}s"
            s_mean = s_val.rolling(win, closed=rolling_closed, min_periods=rolling_min_periods).mean().reindex(t)
            s_std  = s_val.rolling(win, closed=rolling_closed, min_periods=rolling_min_periods).std().reindex(t)

            feat = pd.DataFrame(
                {
                    f"{col}_mean_{sec}s": s_mean.values,
                    f"{col}_std_{sec}s":  s_std.values,
                },
                index=df_work.index,
            )
            out_chunks.append(feat)
            new_columns += list(feat.columns)

        # (2) momentum max/min
        for sec in (windows_rate_sec or []):
            win = f"{sec}s"
            mom_up = rate_per_sec.rolling(win, closed=rolling_closed, min_periods=rolling_min_periods).max().reindex(t)
            mom_dn = rate_per_sec.rolling(win, closed=rolling_closed, min_periods=rolling_min_periods).min().reindex(t)

            feat = pd.DataFrame(
                {
                    f"{col}_momentum_max_up_{sec}s":   mom_up.values,
                    f"{col}_momentum_max_down_{sec}s": mom_dn.values,
                },
                index=df_work.index,
            )
            out_chunks.append(feat)
            new_columns += list(feat.columns)

        if out_chunks:
            feat_df = pd.concat(out_chunks, axis=1).replace([np.inf, -np.inf], np.nan)
            df_work.loc[:, feat_df.columns] = feat_df

    return (df_work, new_columns) if return_df else new_columns

def create_spike_weighted_target(
    df: pd.DataFrame,
    *,
    # ---- ì…ë ¥ ì»¬ëŸ¼ ----
    time_col: str,
    nox_col: str,
    # ---- ì¶œë ¥ ì»¬ëŸ¼ ----
    target_col: str = "target",
    weight_col: str = "weights",
    is_spike_col: str = "is_spike",
    flag_interval_hit_col: str = "flag_interval_hit",
    has_target_col: str = "has_target",
    # ---- íŒŒë¼ë¯¸í„° ----
    delta_sec: int = 150,
    step_sec: int = 5,
    low_thr: float = 20.0,
    high_thr: float = 40.0,
    lookback_sec: int = 150,
    spike_weight: float = 10.0,
    default_weight: float = 1.0,
    return_flags: bool = True,
    verbose: bool = True,
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    ê¸°ì¡´ ë™ì‘ ë™ì¼í•˜ë˜, ëª¨ë“  ì¤‘ê°„/ì¶œë ¥ ì»¬ëŸ¼ëª…ì„ íŒŒë¼ë¯¸í„°ë¡œ ì£¼ì… ê°€ëŠ¥í•˜ê²Œ ìˆ˜ì •.
    ë°˜í™˜: (ê°€ì¤‘ì¹˜/íƒ€ê¹ƒ/í”Œë˜ê·¸ê°€ ì¶”ê°€ëœ df, intervals_df)
    """

    # ê¸°ì¡´ ë³´ì¡° ì—´ ì •ë¦¬ (ì¡´ì¬í•˜ë©´ ì‚­ì œ)
    for col in (target_col, weight_col, is_spike_col, flag_interval_hit_col, has_target_col):
        if col in df.columns:
            del df[col]

    # 1) target ìƒì„± (ë¯¸ë˜ê°’ì„ í˜„ì¬ ì‹œê°ìœ¼ë¡œ ë‹¹ê²¨ ë¶™ì„)
    target_df = df[[time_col, nox_col]].copy()
    target_df[time_col] = target_df[time_col] - pd.Timedelta(seconds=delta_sec)
    target_df = target_df.rename(columns={nox_col: target_col})
    df = pd.merge(df, target_df, on=time_col, how="left")

    t = pd.to_datetime(df[time_col])
    s_nox = df[nox_col]

    # 2) ì—°ì† > high_thr ë¸”ë¡ ì°¾ê¸°
    high_mask = s_nox > high_thr
    high_times = t[high_mask]
    gid = (high_times.diff() > pd.Timedelta(seconds=step_sec)).cumsum()

    lookback = pd.Timedelta(seconds=lookback_sec)

    intervals = []  # ì›ë³¸ êµ¬ê°„(ì‹œí”„íŠ¸ ì „)
    low_mask = s_nox < low_thr

    # ê° >high_thr ë¸”ë¡ë§ˆë‹¤ ìŠ¤íŒŒì´í¬ ê°€ì¤‘ êµ¬ê°„ ìƒì„±
    if not high_times.empty:
        grouped = pd.Series(high_times.values, index=high_times.values).groupby(gid.values)
        for _, grp in grouped:
            t_high_first = pd.to_datetime(grp.iloc[0])  # >high_thr ì§„ì… ì‹œê°
            win_start = t_high_first - lookback
            win_end   = t_high_first

            win_mask = (t >= win_start) & (t < win_end) & low_mask
            if not win_mask.any():
                continue

            t_low_first = t[win_mask].iloc[0]
            low_val = float(s_nox.loc[t == t_low_first].iloc[0]) if (t == t_low_first).any() else np.nan

            intervals.append({
                "t_low_first": t_low_first,
                "t_high_first": t_high_first,
                "low_value_at_first": low_val
            })

    # 3) ê°€ì¤‘ì¹˜/ìŠ¤íŒŒì´í¬ ë¶€ì—¬ (íƒ€ê²Ÿ ì •ë ¬ì— ë§ì¶° êµ¬ê°„ ì‹œí”„íŠ¸)
    df[weight_col] = default_weight
    if return_flags:
        df[flag_interval_hit_col] = False
        df[has_target_col] = df[target_col].notna()

    is_spike = np.zeros(len(df), dtype=bool)

    # intervals_df êµ¬ì„±(ì‹œí”„íŠ¸ ì „/í›„, ê¸¸ì´, íˆíŠ¸ ì¹´ìš´íŠ¸ í¬í•¨)
    intervals_df = pd.DataFrame(intervals)
    if not intervals_df.empty:
        intervals_df["shifted_start"] = intervals_df["t_low_first"]  - pd.Timedelta(seconds=delta_sec)
        intervals_df["shifted_end"]   = intervals_df["t_high_first"] - pd.Timedelta(seconds=delta_sec)
        intervals_df["duration_sec"]  = (intervals_df["t_high_first"] - intervals_df["t_low_first"]).dt.total_seconds()

        # ì „ì²´ hit ë§ˆìŠ¤í¬ ê³„ì‚°ê³¼ ë™ì‹œì— intervals_dfì— n_rows_hit ì±„ìš°ê¸°
        hit_global = np.zeros(len(df), dtype=bool)
        n_rows_hit = []
        for _, row in intervals_df.iterrows():
            lo_s = row["shifted_start"]; hi_s = row["shifted_end"]
            hit_i = (t >= lo_s) & (t <= hi_s)
            n_rows_hit.append(int(hit_i.sum()))
            hit_global |= hit_i

        intervals_df["n_rows_hit"] = n_rows_hit

        # ìœ íš¨ íƒ€ê²Ÿì´ ìˆëŠ” ìœ„ì¹˜ë§Œ ìµœì¢… ë°˜ì˜
        valid = df[target_col].notna()
        final = hit_global & valid
        df.loc[final, weight_col] = spike_weight
        is_spike = final

        if return_flags:
            df.loc[hit_global, flag_interval_hit_col] = True

        intervals_df = intervals_df.reset_index(drop=True)
        intervals_df.insert(0, "interval_id", np.arange(len(intervals_df)))
    else:
        intervals_df = pd.DataFrame(columns=[
            "interval_id","t_low_first","t_high_first","shifted_start","shifted_end",
            "duration_sec","n_rows_hit","low_value_at_first"
        ])

    # is_spike ì»¬ëŸ¼
    df[is_spike_col] = is_spike

    # 4) ìš”ì•½ ì¶œë ¥(ì˜µì…˜)
    if verbose:
        n_nan = int(df[target_col].isna().sum())
        vc = df[weight_col].value_counts().sort_index()
        print("ğŸ¯ íƒ€ê²Ÿ ìƒì„± ì™„ë£Œ")
        print(f"   - ê²°ì¸¡ íƒ€ê²Ÿ: {n_nan:,}ê°œ")
        print("âš–ï¸ ê°€ì¤‘ì¹˜ ë¶„í¬:")
        for w, c in vc.items():
            print(f"     ê°€ì¤‘ì¹˜ {w}: {c:,}ê°œ ({c/len(df)*100:.1f}%)")
        print(f"ğŸ” {is_spike_col}=True: {int(df[is_spike_col].sum()):,}ê°œ "
              f"({df[is_spike_col].mean()*100:.1f}%)")
        print(f"ğŸ“¦ intervals_df: {len(intervals_df)}ê°œ êµ¬ê°„")

    return df, intervals_df
    
# === ì „ì²˜ë¦¬ìš© Class ===

# -----------------------------
# ê³µí†µ: ìš”ì•½í†µê³„/ëª¨ë©˜í…€ ìƒì„±
# -----------------------------
@dataclass
class LGBMFeaturePreprocessor:
    # cfgëŠ” _LGBMWindowsMixin + CommonPreprocessingConfigë¥¼ í¬í•¨í•˜ëŠ” ì–´ë–¤ ì„¤ì •ì´ë“  OK
    # (ì˜ˆ: LGBMTrainPreprocessingConfig, LGBMInferPreprocessingConfig)
    cfg: "_LGBMWindowsMixin"

    def make_interval_features(
        self,
        df: pd.DataFrame,
        *,
        columns: Optional[Sequence[str]] = None,
        return_df: bool = True,
        rolling_closed: str | None = None,
        rolling_min_periods: int | None = None,
        coerce_numeric: bool = True,
    ) -> Tuple[pd.DataFrame, List[str]]:
        if not columns:
            columns = self.cfg.column_config.lgbm_feature_columns  # ë¹„ë©´ ColumnConfigì—ì„œ ValueError

        df2, new_cols = generate_interval_summary_features_time(
            df,
            datetime_col=self.cfg.column_config.col_datetime,
            columns=list(columns),
            windows_summary_sec=self.cfg.windows_summary_sec,
            windows_rate_sec=self.cfg.windows_rate_sec,
            coerce_numeric=coerce_numeric,
            return_df=return_df,
            rolling_closed=rolling_closed,
            rolling_min_periods=rolling_min_periods,
        )
        return df2, new_cols


# -----------------------------------------
# í•™ìŠµ ì „ìš©: íƒ€ê¹ƒ/ê°€ì¤‘ì¹˜ + í•™ìŠµ í”„ë ˆì„ ì •ë¦¬
# -----------------------------------------
@dataclass
class LGBMTrainPreprocessor:
    cfg: "LGBMTrainPreprocessingConfig"

    def make_spike_weighted_target(
        self,
        df: pd.DataFrame,
        *,
        apply_high_nox_weight: bool = True,
        high_nox_basis: Literal["current", "target"] = "target",
        return_flags: bool = True,
        verbose: bool = True,
    ) -> Tuple[pd.DataFrame, pd.DataFrame]:
        cc = self.cfg.column_config

        out_df, intervals_df = create_spike_weighted_target(
            df=df,  # ì§ì „ ë‹¨ê³„ì—ì„œ copy í–ˆë‹¤ë©´ ì—¬ê¸°ì„  X
            time_col=cc.col_datetime,
            nox_col=cc.col_nox,  # ë˜ëŠ” cc.target_column
            target_col=cc.col_lgbm_tmp_target_shift,
            weight_col=cc.col_lgbm_tmp_weight,
            is_spike_col=cc.col_lgbm_tmp_is_spike,
            flag_interval_hit_col=cc.col_lgbm_tmp_flag_interval_hit,
            has_target_col=cc.col_lgbm_tmp_has_target,
            delta_sec=self.cfg.weight_spike_delta_sec,
            step_sec=self.cfg.weight_spike_step_sec,
            low_thr=self.cfg.weight_spike_thr_low,
            high_thr=self.cfg.weight_spike_thr_high,
            lookback_sec=self.cfg.weight_spike_lookback_sec,
            spike_weight=self.cfg.weight_spike_pos,
            default_weight=self.cfg.weight_spike_neg,
            return_flags=return_flags,
            verbose=verbose,
        )
        return out_df, intervals_df

    def prepare_training_frame(
        self,
        df: pd.DataFrame,
        *,
        stat_feature_cols: Sequence[str],          # ê³µí†µ ì „ì²˜ë¦¬ì—ì„œ ë°˜í™˜ëœ new_cols
        extra_feature_cols: Sequence[str] = (),    # í•„ìš” ì‹œ ì¶”ê°€ í”¼ì²˜
        drop_missing: bool = True,
        debug_nan: bool = False,
        verbose: bool = True,
        apply_high_nox_weight: bool = True,
        high_nox_basis: Literal["target","current"] = "target"
    ) -> Tuple[pd.DataFrame, List[str], pd.Series]:
        cc = self.cfg.column_config

        # 1) feature ëª©ë¡ êµ¬ì„±
        base_features: List[str] = list(cc.lgbm_feature_columns)
        feature_cols: List[str] = list(dict.fromkeys([*base_features, *stat_feature_cols, *extra_feature_cols]))

        # 2) íƒ€ê¹ƒ NaN ì œê±°
        before_target = df.shape[0]
        df = df.loc[df[cc.col_lgbm_tmp_target_shift].notna(), :].copy()
        after_target = df.shape[0]
        if verbose:
            print(f"ğŸ¯ íƒ€ê¹ƒ ì¡´ì¬ í–‰ë§Œ ì‚¬ìš©: {before_target:,} â†’ {after_target:,} (ì œê±° {before_target - after_target:,})")

        keep_cols = [
            cc.col_datetime, cc.col_lgbm_tmp_target_shift,
            cc.col_lgbm_tmp_is_spike, cc.col_lgbm_tmp_weight
        ] + feature_cols

        missing = [c for c in keep_cols if c not in df.columns]
        if missing:
            raise KeyError(f"[prepare_training_frame] ëˆ„ë½ëœ ì»¬ëŸ¼: {missing}")

        # 3) df_model êµ¬ì„±
        df_model = df[keep_cols].sort_values(by=cc.col_datetime).reset_index(drop=True)
        if verbose:
            print(f"âœ… ê¸°ë³¸ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ (í–‰: {len(df_model):,}, ì—´: {len(df_model.columns)})")

        # 4) valid_idx ê³„ì‚° ë° (ì˜µì…˜) ë“œë
        valid_cols = feature_cols + [cc.col_lgbm_tmp_target_shift, cc.col_lgbm_tmp_weight]
        valid_idx = df_model[valid_cols].notna().all(axis=1)

        # (2025-09-08) (To-Do) logger ì¶”ê°€í•˜ê³  debug ëª¨ë“œì—ì„œë§Œ ì¶œë ¥í•˜ë„ë¡ ë³€ê²½
        # if debug_nan:
        #     na = df_model[valid_cols].isna()
        #     print("\n[NaN ì§„ë‹¨] ë“œë ì˜ˆì • í–‰ ìˆ˜:", int(na.any(axis=1).sum()))
        #     print("[NaN ì§„ë‹¨] ì»¬ëŸ¼ë³„ NaN ê°œìˆ˜ (Top 20):")
        #     print(na.sum().sort_values(ascending=False).head(20))

        #     base_features = list(cc.lgbm_feature_columns)
        #     stat_features = [c for c in feature_cols if c not in base_features]
        #     print("\n[NaN ì§„ë‹¨] ê·¸ë£¹ë³„ NaN ìœ ë°œ í–‰ ìˆ˜:")
        #     print(" - base_features any NaN:", int(na[base_features].any(axis=1).sum()))
        #     print(" - stat_features any NaN:", int(na[stat_features].any(axis=1).sum()))
        #     print(" - target NaN:", int(na[cc.col_lgbm_tmp_target_shift].sum()))
        #     print(" - weight NaN:", int(na[cc.col_lgbm_tmp_weight].sum()))

        #     bad_ix = na.any(axis=1)
        #     bad_rows = df_model.index[bad_ix][:5]
        #     for i in bad_rows:
        #         bad_cols = na.columns[na.iloc[i]].tolist()
        #         print(f" - row {i} NaN in: {bad_cols[:10]}{' ...' if len(bad_cols)>10 else ''}")

        if drop_missing:
            before = len(df_model)
            df_model = df_model.loc[valid_idx].reset_index(drop=True)
            after = len(df_model)
            if verbose:
                removed = before - after
                pct = (removed / before * 100) if before else 0.0
                print(f"   ì „ì²´ í–‰: {before:,} â†’ {after:,}")
                print(f"   ì œê±°ëœ í–‰: {removed:,} ({pct:.1f}%)")

        # 5) â† ë ˆê±°ì‹œ ë™ì¼ ìœ„ì¹˜: ë“œë ì´í›„, ê³ ë†ë„ ì¶”ê°€ ê°€ì¤‘ì¹˜ ì ìš©
        if apply_high_nox_weight:
            wcol = cc.col_lgbm_tmp_weight
            basis_col = cc.col_lgbm_tmp_target_shift if high_nox_basis == "target" else cc.col_nox
            lo, hi = self.cfg.weight_high_nox_bound_lower, self.cfg.weight_high_nox_bound_upper

            mask = (
                (df_model[wcol] == self.cfg.weight_spike_neg) &  # ê¸°ë³¸ ê°€ì¤‘ì¹˜(=1)ì¸ ê³³ë§Œ
                (df_model[basis_col] > lo) &
                (df_model[basis_col] < hi)
            )
            if mask.any():
                df_model.loc[mask, wcol] = self.cfg.weight_high_nox
                if verbose:
                    print(f"âš–ï¸ ê³ ë†ë„ ì¶”ê°€ ê°€ì¤‘ì¹˜ ì ìš©(ë“œë ì´í›„): {int(mask.sum()):,}í–‰ â†’ {self.cfg.weight_high_nox}")

        return df_model, feature_cols, valid_idx