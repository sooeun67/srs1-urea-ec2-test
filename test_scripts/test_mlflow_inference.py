"""
test_mlflow_inference.py

ê¸°ëŠ¥: (1) RUN_IDë¡œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ(ë˜ëŠ” ìµœì‹  RUN ìë™ ì„ íƒ),
     (2) Influxì—ì„œ ìµœê·¼ ë°ì´í„° ì¡°íšŒ, (3) ê°„ë‹¨ ì „ì²˜ë¦¬,
     (4) ëª¨ë¸ ë¡œë“œ/ì¶”ë¡ , (5) ì½˜ì†”ì— ê²°ê³¼/ì¤‘ê°„ê°’ ì¶œë ¥

í™˜ê²½ë³€ìˆ˜(ê¶Œì¥):
  - MLFLOW_TRACKING_URI: ì˜ˆ) http://10.250.109.206:5000
  - RUN_ID: ì§€ì • ì‹œ í•´ë‹¹ RUN ì‚¬ìš©, ë¯¸ì§€ì • ì‹œ ìµœì‹  RUN ìë™ ì„ íƒ
  - MLFLOW_EXPERIMENT_NAME: ìµœì‹  RUN ìë™ ì„ íƒ ì‹œ í•„ìš” (ì˜ˆ: urea_gp_prod)

  - MODEL_LOCAL_DIR: ìˆ˜ë™ ë³µì‚¬í•œ ëª¨ë¸ ë””ë ‰í† ë¦¬ ì§€ì • ì‹œ MLflow ë‹¤ìš´ë¡œë“œ ìš°íšŒ
    (ë¯¸ì§€ì • ì‹œ, í”„ë¡œì íŠ¸ ë‚´ mlflow_artifacts/<RUN_ID>/urea_gp_model ê²½ë¡œ ìë™ íƒìƒ‰)

  - START_TIME (UTC): ì ˆëŒ€ ì‹œì‘ì‹œê°(UTC) ì§€ì • ì‹œ ì‚¬ìš© (ì˜ˆ: "2025-08-27 00:00:01")
    - ì§€ì • ì‹œ: [START_TIME, START_TIME + INFLUX_WINDOW] êµ¬ê°„ë§Œ ì¡°íšŒ (UTC)
  - START_TIME_KST: ì ˆëŒ€ ì‹œì‘ì‹œê°(KST) ì§€ì • ì‹œ ì‚¬ìš© (ì˜ˆ: "2025-08-27 09:00:01")
    - ì§€ì • ì‹œ: [START_TIME_KST, START_TIME_KST + INFLUX_WINDOW] êµ¬ê°„ë§Œ ì¡°íšŒ (KST)
    - ë¯¸ì§€ì • ì‹œ: [now() - INFLUX_WINDOW, now()] êµ¬ê°„ ì¡°íšŒ

  - INFLUX_HOST (ê¸°ë³¸: 10.238.27.132)
  - INFLUX_PORT (ê¸°ë³¸: 8086)
  - INFLUX_USERNAME (ê¸°ë³¸: read_user)
  - INFLUX_PASSWORD (ê¸°ë³¸: !Skepinfluxuser25)
  - INFLUX_DB (ê¸°ë³¸: SRS1)
  - INFLUX_MEASUREMENT (ê¸°ë³¸: SRS1)
  - INFLUX_WINDOW (ê¸°ë³¸: 10m)
  - INFLUX_LIMIT (ê¸°ë³¸: 120)
"""

import os
import sys
from pathlib import Path
from typing import Optional, List

# Ensure project root is on sys.path regardless of current working directory
THIS_DIR = Path(__file__).resolve().parent
PROJECT_ROOT = THIS_DIR.parent
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

import numpy as np
import pandas as pd
import joblib

from influxdb import InfluxDBClient
import mlflow
import time
import requests

# GP ì œì–´ ëª¨ë¸ ì…ë ¥ ìš”êµ¬ ì»¬ëŸ¼(8ê°œ)
REQUIRED_COLUMNS: List[str] = [
    "_time_gateway",
    "BR1_EO_O2_A",
    "SNR_PMP_UW_S_1",
    "ICF_CCS_FG_T_1",
    "ICF_SCS_FG_T_1",
    "ICF_TMS_NOX_A",
    "ACC_SNR_AI_1A",
    "ACT_STATUS",
]


def get_env(name: str, default: Optional[str] = None) -> str:
    v = os.environ.get(name)
    return v if v is not None else ("" if default is None else str(default))


def select_run_id() -> str:
    run_id = os.environ.get("RUN_ID")
    if run_id:
        print(f"[INFO] í™˜ê²½ë³€ìˆ˜ RUN_ID ì§€ì •ë¨: {run_id}")
        return run_id

    # ê¸°ë³¸ ì‹¤í—˜ëª…ê³¼ Run Name ì ‘ë‘ì–´
    experiment = os.environ.get("MLFLOW_EXPERIMENT_NAME", "skep-urea")
    run_name_prefix = os.environ.get("MLFLOW_RUN_NAME_PREFIX", "urea-SRS1-")

    # Run Nameìœ¼ë¡œ í•„í„°ë§í•˜ì—¬ ìµœì‹  RUN ì„ íƒ
    filter_string = f"tags.mlflow.runName LIKE '{run_name_prefix}%'"

    print(f"[INFO] ìµœì‹  RUN ìë™ ì„ íƒ - ì‹¤í—˜ëª…: {experiment}")
    print(f"[INFO] í•„í„°: {filter_string}")
    exp = mlflow.get_experiment_by_name(experiment)
    if exp is None:
        raise ValueError(f"Experiment '{experiment}' not found")
    runs = mlflow.search_runs(
        experiment_ids=[exp.experiment_id],
        filter_string=filter_string,
        order_by=["start_time DESC"],
        max_results=1,
    )
    if runs.empty:
        raise ValueError(
            f"No runs found in experiment '{experiment}' with filter '{filter_string}'"
        )
    return runs.iloc[0]["run_id"]


def download_model(run_id: str, model_name: str = "urea_gp_model") -> Path:
    # 0) ë¡œì»¬ ê²½ë¡œ ì˜¤ë²„ë¼ì´ë“œ(ë¬¸ì œì‹œ ìˆ˜ë™ ë°°í¬ íŒŒì¼ ì‚¬ìš©)
    override = os.environ.get("MODEL_LOCAL_DIR")
    if override:
        p = Path(override)
        if p.exists() and any(p.rglob("*")):
            print(f"[INFO] ë¡œì»¬ ëª¨ë¸ ê²½ë¡œ ì‚¬ìš©(MODEL_LOCAL_DIR): {p}")
            return p

    # 0-1) í”„ë¡œì íŠ¸ ë‚´ ìˆ˜ë™ ë³µì‚¬ë³¸ ìë™ íƒìƒ‰
    # ìš°ì„ ìˆœìœ„: mlflow_artifacts/<RUN_ID>/urea_gp_model â†’ mlflow_artifacts/<RUN_ID>/artifacts/urea_gp_model â†’ mlflow_artifacts/<RUN_ID>
    local_candidates = [
        PROJECT_ROOT / "mlflow_artifacts" / run_id / model_name,
        PROJECT_ROOT / "mlflow_artifacts" / run_id / "artifacts" / model_name,
        PROJECT_ROOT / "mlflow_artifacts" / run_id,
    ]
    for cand in local_candidates:
        if cand.exists() and any(cand.rglob("*")):
            print(f"[INFO] ë¡œì»¬ ëª¨ë¸ ê²½ë¡œ ìë™ ê°ì§€: {cand}")
            return cand

    dst = Path("/tmp/mlflow_models") / f"{run_id}_{model_name}"
    # ìºì‹œ ì¡´ì¬ ì‹œ ì¬ì‚¬ìš©
    if dst.exists() and any(dst.rglob("*")):
        print(f"[INFO] ìºì‹œëœ ëª¨ë¸ ì‚¬ìš©: {dst}")
        return dst

    print(f"[INFO] ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘: run_id={run_id}, artifact={model_name}")
    t0 = time.time()
    path = mlflow.artifacts.download_artifacts(
        artifact_uri=f"runs:/{run_id}/{model_name}",
        dst_path=str(dst),
    )
    elapsed = time.time() - t0
    print(f"[INFO] ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ ({elapsed:.1f}s): {path}")
    print("[INFO] í¬í•¨ íŒŒì¼ ëª©ë¡(ìµœëŒ€ 10ê°œ):")
    cnt = 0
    for p in Path(path).rglob("*"):
        print(" -", p)
        cnt += 1
        if cnt >= 10:
            print(" - ...")
            break
    return path


def test_mlflow_connection() -> None:
    """Quick MLflow connectivity test with short timeouts.

    - GET tracking root (5s)
    - POST experiments/list (5s)
    - If RUN_ID is set, POST runs/get to print artifact_uri (5s)
    """
    tracking = os.environ.get("MLFLOW_TRACKING_URI")
    print("\nğŸ§ª MLflow ì—°ê²° í…ŒìŠ¤íŠ¸")
    if not tracking:
        print("[WARN] MLFLOW_TRACKING_URI ë¯¸ì„¤ì • â†’ ì—°ê²° í…ŒìŠ¤íŠ¸ ê±´ë„ˆëœ€")
        return

    base = tracking.rstrip("/")
    try:
        r = requests.get(base, timeout=5)
        print(f"  â†³ GET {base} â†’ HTTP {r.status_code}")
    except Exception as e:
        print(f"âŒ GET {base} ì‹¤íŒ¨: {e}")

    try:
        url = f"{base}/api/2.0/mlflow/experiments/list"
        r = requests.post(url, json={}, timeout=5)
        print(f"  â†³ POST /experiments/list â†’ HTTP {r.status_code}")
    except Exception as e:
        print(f"âŒ POST /experiments/list ì‹¤íŒ¨: {e}")

    run_id = os.environ.get("RUN_ID")
    if run_id:
        try:
            url = f"{base}/api/2.0/mlflow/runs/get"
            r = requests.post(url, json={"run_id": run_id}, timeout=5)
            if r.ok:
                data = r.json()
                art = data.get("run", {}).get("info", {}).get("artifact_uri")
                print(f"ğŸ“¦ run.artifact_uri: {art}")
            else:
                print(f"âš ï¸ runs/get HTTP {r.status_code}")
        except Exception as e:
            print(f"âŒ POST /runs/get ì‹¤íŒ¨: {e}")
    else:
        print("â„¹ï¸ RUN_ID ë¯¸ì„¤ì • â†’ runs/get ìƒëµ")


def pick_model_file(root: Path) -> Path:
    candidates: List[Path] = [*root.rglob("*.joblib"), *root.rglob("*.pkl")]
    if not candidates:
        raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {root}")
    print("[INFO] ë¡œë“œ í›„ë³´:")
    for c in candidates:
        print(" -", c)
    return candidates[0]


def query_recent_influx() -> pd.DataFrame:
    host = get_env("INFLUX_HOST", "10.238.27.132")
    port = int(get_env("INFLUX_PORT", "8086"))
    username = get_env("INFLUX_USERNAME", "read_user")
    password = get_env("INFLUX_PASSWORD", "!Skepinfluxuser25")
    database = get_env("INFLUX_DB", "SRS1")
    measurement = get_env("INFLUX_MEASUREMENT", "SRS1")
    # ìš”êµ¬ì‚¬í•­: ìµœê·¼ 20ì´ˆ ì¡°íšŒ (ì´ˆë‹¹ 1í¬ì¸íŠ¸ ê°€ì • â†’ 20ê°œ) ë˜ëŠ” ì ˆëŒ€ ì‹œì‘ì‹œê° ê¸°ë°˜ ì¡°íšŒ
    window = get_env("INFLUX_WINDOW", "20s")
    limit = int(get_env("INFLUX_LIMIT", "200"))
    start_time_kst = get_env("START_TIME_KST", "").strip()
    start_time_utc = get_env("START_TIME", "").strip()

    client = InfluxDBClient(
        host=host,
        port=port,
        username=username,
        password=password,
        database=database,
        timeout=30,
    )

    # ì ˆëŒ€ ì‹œì‘ì‹œê°ì´ ì§€ì •ë˜ë©´ í•´ë‹¹ êµ¬ê°„ë§Œ ì¡°íšŒ (ìš°ì„ ìˆœìœ„: START_TIME(UTC) > START_TIME_KST)
    if start_time_utc:
        # UTC ê¸°ì¤€ ê³ ì • êµ¬ê°„
        start_utc_dt = pd.to_datetime(start_time_utc, utc=True, errors="coerce")
        # INFLUX_WINDOW íŒŒì‹± (s/m)
        w = window.lower().strip()
        secs = 20
        if w.endswith("s"):
            secs = int(w[:-1] or 0)
        elif w.endswith("m"):
            secs = int(w[:-1] or 0) * 60
        else:
            # fallback: 20s
            secs = 20
        end_utc_dt = start_utc_dt + pd.to_timedelta(max(secs - 1, 0), unit="s")
        start_utc = start_utc_dt.strftime("%Y-%m-%dT%H:%M:%SZ")
        end_utc = end_utc_dt.strftime("%Y-%m-%dT%H:%M:%SZ")
        print(
            f"[INFO] ì ˆëŒ€ ì‹œê°„ ì¡°íšŒ(UTC): {start_utc_dt} ~ {end_utc_dt} (window={window})"
        )
        query = (
            f'\nSELECT * FROM "{measurement}" '
            f"WHERE time >= '{start_utc}' AND time <= '{end_utc}' "
            f"ORDER BY time ASC LIMIT {limit}\n"
        )
    elif start_time_kst:
        try:
            start_kst = pd.to_datetime(start_time_kst).tz_localize("Asia/Seoul")
        except Exception:
            start_kst = pd.to_datetime(start_time_kst).tz_convert("Asia/Seoul")
        # INFLUX_WINDOW íŒŒì‹± (s/m)
        w = window.lower().strip()
        secs = 20
        if w.endswith("s"):
            secs = int(w[:-1] or 0)
        elif w.endswith("m"):
            secs = int(w[:-1] or 0) * 60
        else:
            # fallback: 20s
            secs = 20
        # ì¢…ë£Œ ì‹œì  í¬í•¨ ì¡°ê±´(<=)ì´ë¯€ë¡œ ì •í™•íˆ 20ì´ˆ êµ¬ê°„ì„ ë§Œë“¤ê¸° ìœ„í•´ 1ì´ˆ ê°ì†Œ
        end_kst = start_kst + pd.to_timedelta(max(secs - 1, 0), unit="s")
        start_utc = start_kst.tz_convert("UTC").strftime("%Y-%m-%dT%H:%M:%SZ")
        end_utc = end_kst.tz_convert("UTC").strftime("%Y-%m-%dT%H:%M:%SZ")
        print(f"[INFO] ì ˆëŒ€ ì‹œê°„ ì¡°íšŒ(KST): {start_kst} ~ {end_kst} (window={window})")
        query = (
            f'\nSELECT * FROM "{measurement}" '
            f"WHERE time >= '{start_utc}' AND time <= '{end_utc}' "
            f"ORDER BY time ASC LIMIT {limit}\n"
        )
    else:
        query = (
            f'\nSELECT * FROM "{measurement}" '
            f"WHERE time >= now() - {window} AND time <= now() "
            f"ORDER BY time DESC LIMIT {limit}\n"
        )
    print("ğŸ” Influx ì¿¼ë¦¬:", query)
    result = client.query(query)
    points = list(result.get_points()) if result else []
    print(f"ğŸ“Š ì¡°íšŒ í¬ì¸íŠ¸ ìˆ˜: {len(points)}")

    if not points:
        raise RuntimeError("ìµœê·¼ êµ¬ê°„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì‹œê°„ì°½/ì¸¡ì •ê°’ì„ ì¡°ì •í•˜ì„¸ìš”.")

    df = pd.DataFrame(points)
    print("ğŸ—‚ï¸ ì›ë³¸ ë°ì´í„°í”„ë ˆì„:", df.shape)
    # ì£¼ ê´€ì‹¬ ì»¬ëŸ¼ë§Œ ë¯¸ë¦¬ë³´ê¸°: REQUIRED_COLUMNS + time
    preview_cols = [c for c in ["time", *REQUIRED_COLUMNS] if c in df.columns]
    try:
        print(df[preview_cols].head(20) if preview_cols else df.head(20))
    except Exception:
        print(df.head(20))
    return df


def aggregate_last_20s_to_5s(df: pd.DataFrame) -> pd.DataFrame:
    """ìµœê·¼ 20ì´ˆ ë°ì´í„°ë¥¼ 5ì´ˆ ìœˆë„ìš°ë¡œ ìš”ì•½í•˜ì—¬ 4í–‰ ë°˜í™˜.

    - ì„¼ì„œ ì»¬ëŸ¼: 5ì´ˆ í‰ê· 
    - *_status ì»¬ëŸ¼: ê° ìœˆë„ìš°ì˜ ë§ˆì§€ë§‰ ê°’
    - _time_gateway: ê° ìœˆë„ìš°ì˜ ê²½ê³„ ì‹œê°(ì˜¤ë¥¸ìª½ ë¼ë²¨)
    """
    if "time" not in df.columns:
        raise KeyError("Influx ì‘ë‹µì— 'time' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    # ì‹œê°„ ì²˜ë¦¬ ë° ì •ë ¬ (ì˜¤ë¦„ì°¨ìˆœ â†’ ê·¸ë£¹í•‘ ì•ˆì •í™”)
    ts = pd.to_datetime(df["time"], utc=True, errors="coerce")
    df = df.copy()
    df["_ts"] = ts
    df = df.dropna(subset=["_ts"]).sort_values("_ts")
    df = df.set_index("_ts")

    # í•„ìš”í•œ 8ê°œ ì»¬ëŸ¼ë§Œ ì¶”ì¶œ(ì—†ìœ¼ë©´ ì—ëŸ¬)
    needed = [c for c in REQUIRED_COLUMNS if c != "_time_gateway"]
    missing = [c for c in needed if c not in df.columns]
    if missing:
        raise KeyError(f"í•„ìš” ì»¬ëŸ¼ ëˆ„ë½: {missing}")
    sub = df[needed].copy()

    # ì„¼ì„œ/ìƒíƒœ ì»¬ëŸ¼ êµ¬ë¶„
    status_cols = [c for c in sub.columns if c.endswith("_status")]
    sensor_cols = [c for c in sub.columns if c not in status_cols]

    # ê·¸ë£¹í•‘: 5ì´ˆ ê·¸ë£¹, ë¼ë²¨ì€ ì˜¤ë¥¸ìª½ ê²½ê³„
    # ë””ë²„ê·¸: ìœˆë„ìš° ë§¤í•‘ ì •ë³´ ì¶œë ¥(ì›ì‹œ í–‰ â†’ ê° 5ì´ˆ ìœˆë„ìš° ë‚´ í–‰ ê°œìˆ˜)
    win_counts = (
        pd.Series(1, index=df.index)
        .resample("5s", label="right", closed="right")
        .sum()
        .fillna(0)
        .astype(int)
    )
    if not win_counts.empty:
        # ìµœê·¼/ì§€ì • êµ¬ê°„ì˜ ìœˆë„ìš° ë§¤í•‘ ìƒì„¸ ë¡œê·¸ (ìµœëŒ€ 8ê°œ ìœˆë„ìš°)
        idx_sample = win_counts.index[-8:]
        idx_utc = idx_sample
        counts_sample = win_counts.loc[idx_sample].tolist()
        mapping_log = list(zip(idx_utc, counts_sample))
        print("[DEBUG] 5ì´ˆ ìœˆë„ìš°ë³„ ì›ì‹œ í–‰ ê°œìˆ˜(UTC):", mapping_log)
    # ê° ê·¸ë£¹ì— ëŒ€í•´ ì„¼ì„œëŠ” í‰ê· , ìƒíƒœëŠ” ë§ˆì§€ë§‰ ê°’ (ë³´ê°„ ì „ ì›ë³¸)
    df_mean_raw = sub[sensor_cols].resample("5s", label="right", closed="right").mean()
    df_last_raw = (
        sub[status_cols].resample("5s", label="right", closed="right").last()
        if status_cols
        else pd.DataFrame(index=df_mean_raw.index)
    )

    # ë³´ê°„ ì „ ìš”ì•½ ì¶œë ¥
    agg_pre = pd.concat([df_mean_raw, df_last_raw], axis=1)
    agg_pre.index.name = "_time_gateway"
    agg_pre = agg_pre.reset_index()
    try:
        agg_pre["_time_gateway"] = pd.to_datetime(
            agg_pre["_time_gateway"], utc=True, errors="coerce"
        ).dt.tz_convert("UTC")
    except Exception:
        pass
    # ê°€ì¥ ì´ë¥¸ 4ê°œ ìœˆë„ìš°(ì˜ˆ: 05,10,15,20)ë§Œ ìœ ì§€
    agg_pre = agg_pre.sort_values("_time_gateway").head(4)
    print("[INFO] 5ì´ˆ ìœˆë„ìš° ìš”ì•½(ë³´ê°„ ì „):")
    print(agg_pre.tail(4))

    # ì´í›„ ì²˜ë¦¬ìš© ë³µì‚¬ë³¸ì— ë³´ê°„ ìˆ˜í–‰
    df_mean = df_mean_raw.copy()

    # í‰ê· ê°’(ì—°ì†í˜•) ì»¬ëŸ¼ë“¤ì— ëŒ€í•´ NaN ìœˆë„ìš° ffill ì²˜ë¦¬ ë° ë¡œê·¸
    for col in df_mean.columns:
        pre_nan_mask = df_mean[col].isna()
        pre_nan_count = int(pre_nan_mask.sum())
        if pre_nan_count > 0:
            df_mean[col] = df_mean[col].ffill()
            post_nan_mask = df_mean[col].isna()
            post_nan_count = int(post_nan_mask.sum())
            filled_count = pre_nan_count - post_nan_count
            print(
                f"[INFO] {col} 5ì´ˆ í‰ê·  NaN ìœˆë„ìš°: {pre_nan_count} â†’ ffill í›„ {post_nan_count} (ë³´ê°„ëœ ìœˆë„ìš°: {filled_count})"
            )
            if filled_count > 0:
                filled_times = df_mean.index[pre_nan_mask & ~post_nan_mask].tolist()
                sample = filled_times[:5]
                sample_utc = sample
                if len(filled_times) > 5:
                    print(f"[INFO] ë³´ê°„ëœ ìœˆë„ìš° ì˜ˆì‹œ(ìµœëŒ€ 5ê°œ, UTC): {sample_utc} ...")
                else:
                    print(f"[INFO] ë³´ê°„ëœ ìœˆë„ìš°(UTC): {sample_utc}")
    df_last = (
        df_last_raw.copy()
        if not df_last_raw.empty
        else pd.DataFrame(index=df_mean.index)
    )
    # ìƒíƒœê°’(ë²”ì£¼í˜•) ì»¬ëŸ¼ë“¤ì— ëŒ€í•´ì„œë„ ìœˆë„ìš°ê°€ ë¹„ì–´ NaNì´ë©´ ì§ì „ ê°’ìœ¼ë¡œ ffill
    if not df_last.empty:
        for col in df_last.columns:
            pre_nan_mask = df_last[col].isna()
            pre_nan_count = int(pre_nan_mask.sum())
            if pre_nan_count > 0:
                df_last[col] = df_last[col].ffill()
                post_nan_mask = df_last[col].isna()
                post_nan_count = int(post_nan_mask.sum())
                filled_count = pre_nan_count - post_nan_count
                print(
                    f"[INFO] {col} 5ì´ˆ ë§ˆì§€ë§‰ê°’ NaN ìœˆë„ìš°: {pre_nan_count} â†’ ffill í›„ {post_nan_count} (ë³´ê°„ëœ ìœˆë„ìš°: {filled_count})"
                )
                if filled_count > 0:
                    filled_times = df_last.index[pre_nan_mask & ~post_nan_mask].tolist()
                    sample = filled_times[:5]
                    sample_utc = sample
                    if len(filled_times) > 5:
                        print(
                            f"[INFO] ë³´ê°„ëœ ìœˆë„ìš° ì˜ˆì‹œ(ìµœëŒ€ 5ê°œ, UTC): {sample_utc} ..."
                        )
                    else:
                        print(f"[INFO] ë³´ê°„ëœ ìœˆë„ìš°(UTC): {sample_utc}")

    agg = pd.concat([df_mean, df_last], axis=1)
    agg.index.name = "_time_gateway"
    agg = agg.reset_index()

    # Ensure gateway time is displayed in UTC
    try:
        agg["_time_gateway"] = pd.to_datetime(
            agg["_time_gateway"], utc=True, errors="coerce"
        ).dt.tz_convert("UTC")
    except Exception:
        pass

    # ìµœì‹  4ê°œ ìœˆë„ìš°ë§Œ ë‚¨ê¹€ (DESC â†’ ìƒìœ„ 4 â†’ ì‹œê°„ìˆœìœ¼ë¡œ ì¬ì •ë ¬)
    # ê°€ì¥ ì´ë¥¸ 4ê°œ ìœˆë„ìš°(ì˜ˆ: 05,10,15,20)ë§Œ ìœ ì§€
    agg = agg.sort_values("_time_gateway").head(4)

    # ë¡œê·¸ ì¶œë ¥ (ë³´ê°„ í›„)
    print("[INFO] 5ì´ˆ ìœˆë„ìš° ìš”ì•½(ë³´ê°„ í›„):")
    print(agg.tail(4))

    # ì—´ ìˆœì„œ ì •ë ¬: REQUIRED_COLUMNS ìˆœì„œ ìœ ì§€(ì¡´ì¬í•˜ëŠ” ê²ƒë§Œ)
    ordered_cols = [c for c in REQUIRED_COLUMNS if c in agg.columns]
    agg = agg[ordered_cols]
    return agg


def main() -> None:
    print("ğŸš€" + "=" * 58)
    print("ğŸš€ MLflow ëª¨ë¸ ê¸°ë°˜ ì‹¤ì‹œê°„ ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("ğŸš€" + "=" * 58)

    tracking_uri = os.environ.get("MLFLOW_TRACKING_URI")
    if tracking_uri:
        print(f"ğŸ”— MLFLOW_TRACKING_URI: {tracking_uri}")
    else:
        print(
            "âš ï¸ MLFLOW_TRACKING_URIê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. mlflow ê¸°ë³¸ ì„¤ì •ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
        )

    # 1) RUN ì„ íƒ
    if tracking_uri:
        mlflow.set_tracking_uri(tracking_uri)
    # ì—°ê²° ì‚¬ì „ ì ê²€
    test_mlflow_connection()
    run_id = select_run_id()
    print(f"ğŸ·ï¸ ì‚¬ìš© RUN_ID: {run_id}")

    # 2) ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
    model_root = download_model(run_id=run_id, model_name="urea_gp_model")

    # 3) ëª¨ë¸ íŒŒì¼ ì„ íƒ ë° ë¡œë“œ
    model_file = pick_model_file(model_root)
    model = joblib.load(model_file)
    print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_file}")

    # 4) Influx ìµœê·¼ ë°ì´í„° ì¡°íšŒ
    df = query_recent_influx()

    # 5) 5ì´ˆ ìœˆë„ìš° ìš”ì•½(ìµœê·¼ 20ì´ˆ â†’ 4í–‰)
    agg = aggregate_last_20s_to_5s(df)
    print("ğŸ§¾ ëª¨ë¸ ì…ë ¥ìš© ìš”ì•½(ì—´ ìˆœì„œ ê³ ì •):", agg.shape)
    print(agg)

    # 6) ëª¨ë¸ ì…ë ¥í–‰ ë§Œë“¤ê¸°: [Hz, O2, Temp] = [SNR_PMP_UW_S_1, BR1_EO_O2_A, ICF_CCS_FG_T_1]
    feature_cols = ["SNR_PMP_UW_S_1", "BR1_EO_O2_A", "ICF_CCS_FG_T_1"]
    missing_feat = [c for c in feature_cols if c not in agg.columns]
    if missing_feat:
        raise KeyError(f"ëª¨ë¸ ì…ë ¥ í”¼ì²˜ ëˆ„ë½: {missing_feat}")

    X_all = agg[feature_cols]
    valid_mask = ~X_all.isna().any(axis=1)
    invalid_times = agg.loc[~valid_mask, "_time_gateway"].tolist()
    if invalid_times:
        print(
            f"[WARN] ê²°ì¸¡ì¹˜ë¡œ ì¸í•´ ì˜ˆì¸¡ì—ì„œ ì œì™¸ëœ 5ì´ˆ êµ¬ê°„: {len(invalid_times)}ê±´ â†’ {invalid_times}"
        )

    X = X_all.loc[valid_mask].to_numpy(dtype=float)
    valid_times = agg.loc[valid_mask, "_time_gateway"].tolist()
    print("ğŸ§® ì˜ˆì¸¡ ì…ë ¥ ë°°ì—´ í˜•íƒœ:", X.shape)
    print(X)

    # 7) ì˜ˆì¸¡: 5ì´ˆ ìœˆë„ìš° í‰ê·  ì…ë ¥ë§Œ ì‚¬ìš©í•˜ì—¬ ê° ì‹œì ì˜ NOx í‰ê·  ì˜ˆì¸¡ (ê²°ì¸¡ êµ¬ê°„ ì œì™¸)
    if len(X) > 0:
        pred = model.predict(X)
        for t, v in zip(valid_times, pred):
            val = v[0] if hasattr(v, "__len__") else v
            print(f"ğŸ¯ {t} â†’ NOx mean={float(val):.3f}")
    else:
        print("âš ï¸ ì˜ˆì¸¡ ê°€ëŠ¥í•œ(ê²°ì¸¡ ì—†ëŠ”) 5ì´ˆ êµ¬ê°„ì´ ì—†ìŠµë‹ˆë‹¤.")

    # ê²°ì¸¡ìœ¼ë¡œ ì œì™¸ëœ êµ¬ê°„ì€ NaNìœ¼ë¡œ í‘œì‹œ
    for t in invalid_times:
        print(f"âšª {t} â†’ NOx mean=NaN (insufficient data)")

    print("\nğŸ“Œ ìš”ì•½")
    print("- RUN_ID:", run_id)
    print("- ëª¨ë¸ ê²½ë¡œ:", model_file)
    print("- ì…ë ¥ ìš”ì•½ í–‰ ìˆ˜:", len(agg))


if __name__ == "__main__":
    main()
