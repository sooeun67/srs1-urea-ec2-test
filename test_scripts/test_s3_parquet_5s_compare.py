"""
S3 parquet â†’ 5ì´ˆ ìœˆë„ìš° ìš”ì•½ í›„ InfluxDB ê²°ê³¼ì™€ ë¹„êµ

ì‚¬ìš©ë²•(UTC ê³ ì • êµ¬ê°„ ì˜ˆ):
  export START_TIME="2025-08-27 00:00:01"  # UTC ì‹œì‘ ì‹œê°
  python test_scripts/test_s3_parquet_5s_compare.py

í™˜ê²½ë³€ìˆ˜:
  - S3_URI: S3 ê°ì²´ ê²½ë¡œ (ê¸°ë³¸: ì˜ˆì‹œ íŒŒì¼)
  - START_TIME (UTC) ë˜ëŠ” START_TIME_KST (KST): ê³ ì • êµ¬ê°„ ì‹œì‘
  - INFLUX_HOST/PORT/USERNAME/PASSWORD/DB/MEASUREMENT (Influx ë¹„êµìš©, ì„ íƒ)
  - INFLUX_WINDOW (ê¸°ë³¸ 20s), INFLUX_LIMIT (ê¸°ë³¸ 200)

ì¶œë ¥:
  - S3 ì›ì‹œ ê°œìš”, 5ì´ˆ ìœˆë„ìš° ìš”ì•½(ë³´ê°„ ì „/í›„)
  - ì„ íƒ ì‹œ Influx ë™ì¼ êµ¬ê°„ ìš”ì•½
  - ì‹œê°„ í‚¤(_time_gateway) ê¸°ì¤€ìœ¼ë¡œ ì»¬ëŸ¼ë³„ ì°¨ì´(ì ˆëŒ€ê°’) ìš”ì•½
"""

import os
import io
import gzip
from pathlib import Path
from typing import List, Optional

import boto3
import pandas as pd
import numpy as np

try:
    from influxdb import InfluxDBClient
except Exception:  # Influx ë¯¸ì‚¬ìš© ì‹œ í†µê³¼
    InfluxDBClient = None  # type: ignore


REQUIRED_COLUMNS: List[str] = [
    "_time_gateway",
    "BR1_EO_O2_A",
    "SNR_PMP_UW_S_1",
    "ICF_CCS_FG_T_1",
    "ICF_SCS_FG_T_1",
    "ICF_TMS_NOX_A",
    "ACC_SNR_AI_1A",
    "ACT_STATUS",
]


def get_env(name: str, default: Optional[str] = None) -> str:
    v = os.environ.get(name)
    return v if v is not None else ("" if default is None else str(default))


def _read_from_s3(uri: str) -> bytes:
    if uri.startswith("s3://"):
        _, rest = uri.split("s3://", 1)
        bucket, key = rest.split("/", 1)
    else:
        # '/bucket/...' í˜•íƒœë„ í—ˆìš©
        rest = uri.lstrip("/")
        bucket, key = rest.split("/", 1)

    s3 = boto3.client("s3")
    obj = s3.get_object(Bucket=bucket, Key=key)
    body = obj["Body"].read()
    return body


def read_parquet_from_s3(uri: str) -> pd.DataFrame:
    """S3ì—ì„œ ë°ì´í„°ë¥¼ ì½ì–´ DataFrameìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.

    ìš°ì„  parquetìœ¼ë¡œ ì‹œë„í•˜ê³ , ì‹¤íŒ¨ ì‹œ JSON lines â†’ CSV ìˆœìœ¼ë¡œ ìë™ íŒë³„í•©ë‹ˆë‹¤.
    íŒŒì¼ëª…ì´ .gzë©´ gzip í•´ì œ í›„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    import pyarrow.parquet as pq

    raw = _read_from_s3(uri)
    data = raw
    if uri.endswith(".gz"):
        try:
            data = gzip.decompress(raw)
        except Exception:
            data = raw

    # 1) Parquet ì‹œë„
    try:
        table = pq.read_table(io.BytesIO(data))
        df = table.to_pandas(types_mapper=pd.ArrowDtype)
        print("ğŸ“¥ ë¡œë“œ í˜•ì‹: parquet")
        return df
    except Exception:
        pass

    # 2) JSON Lines ì‹œë„
    try:
        text = data.decode("utf-8", errors="replace")
        df_json = pd.read_json(io.StringIO(text), lines=True)
        if isinstance(df_json, pd.DataFrame) and not df_json.empty:
            print("ğŸ“¥ ë¡œë“œ í˜•ì‹: jsonl")
            return df_json
    except Exception:
        pass

    # 3) CSV ì‹œë„
    try:
        text = data.decode("utf-8", errors="replace")
        df_csv = pd.read_csv(io.StringIO(text))
        print("ğŸ“¥ ë¡œë“œ í˜•ì‹: csv")
        return df_csv
    except Exception as e:
        print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ ë˜ëŠ” ì†ìƒëœ íŒŒì¼: {e}")
        raise


def detect_time_column(df: pd.DataFrame) -> str:
    candidates = ["time", "_time", "timestamp", "event_time"]
    for c in candidates:
        if c in df.columns:
            return c
    raise KeyError(f"ì‹œê°„ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í›„ë³´: {candidates}")


def aggregate_5s(df: pd.DataFrame) -> pd.DataFrame:
    time_col = detect_time_column(df)
    ts = pd.to_datetime(df[time_col], utc=True, errors="coerce")
    df = df.copy()
    df["_ts"] = ts
    df = df.dropna(subset=["_ts"]).sort_values("_ts").set_index("_ts")

    needed = [c for c in REQUIRED_COLUMNS if c != "_time_gateway"]
    missing = [c for c in needed if c not in df.columns]
    if missing:
        raise KeyError(f"í•„ìš” ì»¬ëŸ¼ ëˆ„ë½: {missing}")
    sub = df[needed].copy()

    status_cols = [c for c in sub.columns if c.endswith("_status")]
    sensor_cols = [c for c in sub.columns if c not in status_cols]

    # ë³´ê°„ ì „ ìš”ì•½
    mean_raw = sub[sensor_cols].resample("5s", label="right", closed="right").mean()
    last_raw = (
        sub[status_cols].resample("5s", label="right", closed="right").last()
        if status_cols
        else pd.DataFrame(index=mean_raw.index)
    )
    agg_pre = pd.concat([mean_raw, last_raw], axis=1)
    agg_pre.index.name = "_time_gateway"
    agg_pre = agg_pre.reset_index()

    print("ğŸ§¾ S3 5ì´ˆ ìœˆë„ìš° ìš”ì•½(ë³´ê°„ ì „, UTC):")
    print(
        agg_pre.sort_values("_time_gateway").head(4)[
            [c for c in REQUIRED_COLUMNS if c in agg_pre.columns]
        ]
    )

    # ffill
    mean = mean_raw.copy()
    for col in mean.columns:
        if mean[col].isna().any():
            pre = int(mean[col].isna().sum())
            mean[col] = mean[col].ffill()
            post = int(mean[col].isna().sum())
            if pre:
                print(f"ğŸ› ï¸ {col} í‰ê·  NaN: {pre} â†’ {post} (ffill: {pre - post})")
    last = last_raw.copy()
    if not last.empty:
        for col in last.columns:
            if last[col].isna().any():
                pre = int(last[col].isna().sum())
                last[col] = last[col].ffill()
                post = int(last[col].isna().sum())
                if pre:
                    print(f"ğŸ› ï¸ {col} ë§ˆì§€ë§‰ê°’ NaN: {pre} â†’ {post} (ffill: {pre - post})")

    agg = pd.concat([mean, last], axis=1)
    agg.index.name = "_time_gateway"
    agg = agg.reset_index()
    # UTC ìœ ì§€
    agg["_time_gateway"] = pd.to_datetime(
        agg["_time_gateway"], utc=True, errors="coerce"
    )
    agg = agg.sort_values("_time_gateway").head(4)

    print("ğŸ§¾ S3 5ì´ˆ ìœˆë„ìš° ìš”ì•½(ë³´ê°„ í›„, UTC):")
    print(agg[[c for c in REQUIRED_COLUMNS if c in agg.columns]])
    return agg[[c for c in REQUIRED_COLUMNS if c in agg.columns]]


def query_influx_and_aggregate(
    start_utc: pd.Timestamp, secs: int = 20
) -> Optional[pd.DataFrame]:
    if InfluxDBClient is None:
        print("â„¹ï¸ influxdb íŒ¨í‚¤ì§€ê°€ ì—†ì–´ ë¹„êµë¥¼ ìƒëµí•©ë‹ˆë‹¤.")
        return None

    host = get_env("INFLUX_HOST", "10.238.27.132")
    port = int(get_env("INFLUX_PORT", "8086"))
    username = get_env("INFLUX_USERNAME", "read_user")
    password = get_env("INFLUX_PASSWORD", "!Skepinfluxuser25")
    database = get_env("INFLUX_DB", "SRS1")
    measurement = get_env("INFLUX_MEASUREMENT", "SRS1")

    client = InfluxDBClient(
        host=host,
        port=port,
        username=username,
        password=password,
        database=database,
        timeout=30,
    )
    end_utc = start_utc + pd.to_timedelta(max(secs - 1, 0), unit="s")
    q = (
        f'SELECT * FROM "{measurement}" '
        f"WHERE time >= '{start_utc.strftime('%Y-%m-%dT%H:%M:%SZ')}' AND time <= '{end_utc.strftime('%Y-%m-%dT%H:%M:%SZ')}' "
        f"ORDER BY time ASC LIMIT 200"
    )
    print("ğŸ” Influx ì¿¼ë¦¬:", q)
    r = client.query(q)
    pts = list(r.get_points()) if r else []
    print(f"ğŸ“Š Influx ì¡°íšŒ í¬ì¸íŠ¸ ìˆ˜: {len(pts)}")
    if not pts:
        return None
    df = pd.DataFrame(pts)
    return aggregate_5s(df)


def main() -> None:
    print("ğŸ“¦ S3 â†’ 5ì´ˆ ìœˆë„ìš° ìš”ì•½ ë° Influx ë¹„êµ")
    s3_uri = get_env(
        "S3_URI",
        "/zero4-wte-raw-035989641590-2/plantcode=SRS1/ver=1/field=op_sts/Year=2025/MMDD=0827/zero4-wte-ai-kdf-SRS1-op_sts-1-2025-08-27-00-08-47-1389c518-3e20-4c96-82ee-ef058e3f25a4.gz",
    )
    if not s3_uri.startswith("s3://"):
        s3_uri = f"s3://{s3_uri.lstrip('/')}"
    print("ğŸ”— S3_URI:", s3_uri)

    # ì‹œê°„ ë²”ìœ„: START_TIME(UTC ìš°ì„ ) or START_TIME_KST
    window = get_env("INFLUX_WINDOW", "20s").lower().strip()
    secs = 20
    if window.endswith("s"):
        secs = int(window[:-1] or 0)
    elif window.endswith("m"):
        secs = int(window[:-1] or 0) * 60

    start_time_utc = get_env("START_TIME", "").strip()
    start_time_kst = get_env("START_TIME_KST", "").strip()
    if start_time_utc:
        start_dt_utc = pd.to_datetime(start_time_utc, utc=True, errors="coerce")
        print("â±ï¸ ê¸°ì¤€(UTC):", start_dt_utc)
    elif start_time_kst:
        start_dt_utc = (
            pd.to_datetime(start_time_kst).tz_localize("Asia/Seoul").tz_convert("UTC")
        )
        print("â±ï¸ ê¸°ì¤€(KSTâ†’UTC):", start_dt_utc)
    else:
        raise ValueError("START_TIME ë˜ëŠ” START_TIME_KSTë¥¼ ì§€ì •í•˜ì„¸ìš”.")

    # S3 ë¡œë“œ ë° í•„í„°
    df_raw = read_parquet_from_s3(s3_uri)
    print("ğŸ—‚ï¸ S3 ë¡œë“œ DF:", df_raw.shape)
    time_col = detect_time_column(df_raw)
    ts = pd.to_datetime(df_raw[time_col], utc=True, errors="coerce")
    df_raw = df_raw.loc[
        ts.between(
            start_dt_utc, start_dt_utc + pd.to_timedelta(max(secs - 1, 0), unit="s")
        )
    ].copy()
    print("ğŸ“Š S3 í•„í„° í›„ í¬ì¸íŠ¸ ìˆ˜:", len(df_raw))

    # 5ì´ˆ ìš”ì•½ (S3)
    s3_agg = aggregate_5s(df_raw)

    # Influx ë¹„êµ(ì„ íƒ)
    infl_agg = query_influx_and_aggregate(start_dt_utc, secs)
    if infl_agg is None:
        return

    # ë¹„êµ: ì‹œê°„ í‚¤ ê¸°ì¤€ ë‚´ë¶€ ì¡°ì¸
    join_key = "_time_gateway"
    common_cols = [
        c
        for c in REQUIRED_COLUMNS
        if c != join_key and c in s3_agg.columns and c in infl_agg.columns
    ]
    merged = s3_agg.merge(
        infl_agg, on=join_key, how="inner", suffixes=("_s3", "_influx")
    )
    print("\nğŸ“ ë¹„êµ ê²°ê³¼(UTC, ê³µí†µ ì‹œê°„ êµì§‘í•©):", merged.shape)
    if not merged.empty:
        diffs = {}
        for c in common_cols:
            diffs[c] = (merged[f"{c}_s3"] - merged[f"{c}_influx"]).abs()
        diff_df = pd.DataFrame(diffs)
        diff_df.insert(0, join_key, merged[join_key])
        print("\nğŸ“ ì ˆëŒ€ ì°¨ì´(ì»¬ëŸ¼ë³„):")
        print(diff_df)
        print("\nğŸ§® ì»¬ëŸ¼ë³„ í‰ê·  ì ˆëŒ€ì˜¤ì°¨:")
        print(diff_df[common_cols].mean(numeric_only=True))


if __name__ == "__main__":
    main()
